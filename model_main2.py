# -*- coding: utf-8 -*-
"""model_main2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TBeP9tQ6rGi_iRo2JgIPZPD4Pm2TEhs-
"""

import torch
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("Device name:", torch.cuda.get_device_name(0))
else:
    print("No GPU detected.")

!pip install torch-geometric

from google.colab import drive
drive.mount('/content/drive')

import os
path="/content/drive/MyDrive/RAVDESS/patch_embeddings"
res=dict()
count=0
for root, dirs, files in os.walk(path):
  for file in files:
    if file.endswith('.npy'):
        count+=1
print(count)

import torch
import torch.nn as nn
from torchvision import models, transforms
import numpy as np
import cv2
import os
import pickle
import time

# --- Configuration ---
BATCH_SIZE = 64
MICRO_BATCH_SIZE = 128  # New: Process patches in smaller batches

# --- Device and Model Setup ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load pre-trained ResNet-18
resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
resnet = nn.Sequential(*list(resnet.children())[:-1])
resnet.to(device).eval()

# --- Image Transformation Pipeline ---
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# ----------------------------------------------------
# --- Patch Extraction Function (Provided by User) ---
# ----------------------------------------------------
def extract_patches_from_landmarks(frame, landmarks, patch_size=32):
    """
    Extract square patches around each landmark in a frame.
    """
    h, w, _ = frame.shape
    patches = []

    for (x, y) in landmarks:
        cx, cy = int(x), int(y)

        x1 = cx - patch_size // 2
        y1 = cy - patch_size // 2
        x2 = x1 + patch_size
        y2 = y1 + patch_size

        x1c = max(0, x1)
        y1c = max(0, y1)
        x2c = min(w, x2)
        y2c = min(h, y2)

        patch = frame[y1c:y2c, x1c:x2c]

        if patch.size == 0:
            continue

        patch = cv2.resize(patch, (patch_size, patch_size))
        patches.append(patch)

    return patches

# --- Embedding Function ---
def get_patch_embeddings(all_patches, micro_batch_size):
    if not all_patches:
        return np.array([])

    all_features = []
    for i in range(0, len(all_patches), micro_batch_size):
        micro_batch_patches = all_patches[i:i + micro_batch_size]
        if not micro_batch_patches:
            continue

        inputs = torch.stack([transform(p) for p in micro_batch_patches]).to(device)

        with torch.no_grad():
            features = resnet(inputs)
            features = features.squeeze(-1).squeeze(-1)
        all_features.append(features.cpu().numpy())

    if all_features:
        return np.concatenate(all_features, axis=0)
    else:
        return np.array([])

# ----------------------------------------------------
# --- Main Loop (Optimized with Checkpoint) ---
# ----------------------------------------------------
frame_root = "/content/drive/MyDrive/RAVDESS/frames"
landmark_root = "/content/drive/MyDrive/RAVDESS/Landmarks_Processed"
embedding_root = "/content/drive/MyDrive/RAVDESS/patch_embeddings"
os.makedirs(embedding_root, exist_ok=True)

print(f"Starting feature extraction on device: {device} with BATCH_SIZE: {BATCH_SIZE}")
print("-" * 50)

for subfolder in os.listdir(frame_root):
    start_time = time.time()
    frame_dir = os.path.join(frame_root, subfolder)
    landmark_file = os.path.join(landmark_root, subfolder + ".pkl")
    save_path = os.path.join(embedding_root, subfolder + ".npy") # Define save path early

    # **NEW CHECKPOINT LOGIC**
    if os.path.exists(save_path):
        print(f"[SKIP] Embeddings for {subfolder} already exist. Skipping.")
        continue
    # **END NEW CHECKPOINT LOGIC**

    if not os.path.isdir(frame_dir) or not os.path.exists(landmark_file):
        continue

    # Load all landmarks for the video
    try:
        with open(landmark_file, "rb") as f:
            all_landmarks = pickle.load(f)
    except Exception as e:
        print(f"[ERROR] Could not load landmarks for {subfolder}: {e}")
        continue

    frame_files = sorted([f for f in os.listdir(frame_dir) if f.endswith(".jpg")])

    if len(frame_files) != len(all_landmarks):
        print(f"[SKIP] Frame count ({len(frame_files)}) != landmarks length ({len(all_landmarks)}) in {subfolder}")
        continue

    # Buffers for Batching
    batch_frames_data = []
    all_video_embeddings = []

    for idx, fname in enumerate(frame_files):
        frame_path = os.path.join(frame_dir, fname)
        landmarks = np.array(all_landmarks[idx])

        batch_frames_data.append((frame_path, landmarks))

        is_full_batch = len(batch_frames_data) == BATCH_SIZE
        is_last_frame = idx == len(frame_files) - 1

        if is_full_batch or is_last_frame:
            # --- START BATCH PROCESSING ---
            all_patches_in_batch = []
            patch_counts = []

            # 1. CPU: Load frames and extract patches for the batch
            for path, current_landmarks in batch_frames_data:
                frame = cv2.imread(path)
                if frame is None:
                    patch_counts.append(0)
                    continue

                current_patches = extract_patches_from_landmarks(frame, current_landmarks)

                all_patches_in_batch.extend(current_patches)
                patch_counts.append(len(current_patches))

            # 2. GPU: Get embeddings for the massive patch batch
            all_embeddings = get_patch_embeddings(all_patches_in_batch, MICRO_BATCH_SIZE)

            # 3. CPU: Re-aggregate embeddings back to frame structure
            start_idx = 0
            for count in patch_counts:
                if count > 0:
                    frame_embeddings = all_embeddings[start_idx : start_idx + count]
                    all_video_embeddings.append(frame_embeddings)
                    start_idx += count
                else:
                    all_video_embeddings.append(np.array([]))

            # Reset batch buffer
            batch_frames_data = []

            if is_full_batch:
                print(f"  [Progress] Processed batch ending at frame {idx+1}/{len(frame_files)}")

    # 4. Save the aggregated embeddings for the whole video
    if all_video_embeddings:
        try:
            # save_path is defined earlier
            np.save(save_path, np.array(all_video_embeddings, dtype=object))
            end_time = time.time()
            print(f"[SAVED] Embeddings for {subfolder} ({len(frame_files)} frames) â†’ Time: {end_time - start_time:.2f}s")
        except Exception as e:
             print(f"[ERROR] Saving failed for {subfolder}: {e}")

print("-" * 50)
print("Feature extraction complete.")

import torch
if torch.cuda.is_available(): print("yes")
else: print("no")

# # ==========================================
# # FULL AUDIO-VISUAL EMOTION RECOGNITION SCRIPT
# # ==========================================

# # ---------------------------
# # Imports
# # ---------------------------
# import os
# import pickle
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt

# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# from torch.utils.data import Dataset, DataLoader, Subset
# from torch_geometric.data import Data
# from torch_geometric.nn import GCNConv, GATConv
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
# from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence

# # ---------------------------
# # 1. Dataset
# # ---------------------------
# class RAVDESSDataset(Dataset):
#     def __init__(self, data_dir, labels_csv):
#         self.data_dir = data_dir
#         self.labels_df = pd.read_csv(labels_csv)
#         self.files = sorted(os.listdir(data_dir))

#         # Map emotions to numeric labels
#         self.emotion_to_id = {
#             'neutral': 0, 'calm': 1, 'happy': 2, 'sad': 3,
#             'angry': 4, 'fearful': 5, 'disgust': 6, 'surprised': 7
#         }
#         self.labels_df["emotion_label"] = self.labels_df["emotion_label"].map(self.emotion_to_id)
#         self.filename_to_label = dict(zip(self.labels_df["filename"], self.labels_df["emotion_label"]))

#         # Precompute anatomical edge index
#         self.edge_index = torch.tensor(self.get_anatomical_edge_list(), dtype=torch.long).t().contiguous()

#     def __len__(self):
#         return len(self.files)

#     def get_anatomical_edge_list(self):
#         edge_index = []
#         # Right iris
#         for i in range(7): edge_index.append((i, i + 1))
#         edge_index.append((7, 0))
#         # Right eye boundary
#         for i in range(8, 19): edge_index.append((i, i + 1))
#         edge_index.append((19, 8))
#         # Right pupil
#         for i in range(20, 27): edge_index.append((i, i + 1))
#         edge_index.append((27, 20))
#         # Left iris
#         for i in range(28, 35): edge_index.append((i, i + 1))
#         edge_index.append((35, 28))
#         # Left eye boundary
#         for i in range(36, 47): edge_index.append((i, i + 1))
#         edge_index.append((41, 36))
#         # Left pupil
#         for i in range(48, 55): edge_index.append((i, i + 1))
#         edge_index.append((55, 48))
#         # Jawline
#         for i in range(56, 72): edge_index.append((i, i + 1))
#         edge_index.append((72, 56))
#         # Right eyebrow
#         for i in range(73, 77): edge_index.append((i, i + 1))
#         edge_index.append((77, 73))
#         # Left eyebrow
#         for i in range(78, 82): edge_index.append((i, i + 1))
#         edge_index.append((82, 78))
#         # Nose bridge + lower nose
#         for i in range(83, 91): edge_index.append((i, i + 1))
#         edge_index.append((91, 83))
#         # Inner mouth
#         for i in range(104, 115): edge_index.append((i, i + 1))
#         edge_index.append((115, 104))
#         # Outer mouth
#         for i in range(116, 123): edge_index.append((i, i + 1))
#         edge_index.append((123, 116))
#         # Additional anatomical connections
#         edge_index += [(83, 92),(83, 98),(91, 110),(91, 120),(73, 92),(77, 97),
#                        (78, 98),(82, 103),(56, 116),(72, 123),(92, 78),(98, 73),(83, 91),(110, 120)]
#         return edge_index

#     def __getitem__(self, idx):
#         filename = self.files[idx]
#         file_path = os.path.join(self.data_dir, filename)

#         with open(file_path, "rb") as f:
#             video_data = pickle.load(f)

#         frame_graphs = []
#         mfcc_frames = []

#         for frame in video_data:
#             landmarks = np.array(frame['landmarks'], dtype=np.float32)
#             landmarks -= landmarks.mean(axis=0)  # center landmarks
#             mfcc = np.array(frame['mfcc'], dtype=np.float32)
#             mfcc_frames.append(mfcc)

#             x = torch.tensor(landmarks, dtype=torch.float)
#             frame_graphs.append(Data(x=x, edge_index=self.edge_index))

#         # Convert MFCC to tensor
#         mfcc_tensor = torch.tensor(np.stack(mfcc_frames), dtype=torch.float)
#         # Normalize per video
#         mfcc_tensor = (mfcc_tensor - mfcc_tensor.mean(axis=0)) / (mfcc_tensor.std(axis=0) + 1e-6)

#         label = torch.tensor(self.filename_to_label[filename], dtype=torch.long)
#         seq_len = mfcc_tensor.size(0)
#         return mfcc_tensor, frame_graphs, label, seq_len

# # ---------------------------
# # Collate function
# # ---------------------------
# def collate_fn(batch):
#     mfccs, graphs, labels, lengths = zip(*batch)
#     lengths = torch.tensor(lengths, dtype=torch.long)
#     labels = torch.tensor(labels, dtype=torch.long)
#     return mfccs, graphs, labels, lengths

# # ---------------------------
# # 2. Model
# # ---------------------------
# class LandmarkGraphEncoder(nn.Module):
#     def __init__(self, in_dim, hidden_dim, out_dim, heads=4):
#         super().__init__()
#         self.gcn = GCNConv(in_dim, hidden_dim)
#         self.gat = GATConv(hidden_dim, out_dim, heads=heads, concat=False)
#     def forward(self, x, edge_index):
#         x = self.gcn(x, edge_index)
#         x = F.relu(x)
#         x = self.gat(x, edge_index)
#         return x

# class AudioEncoder(nn.Module):
#     def __init__(self, in_dim, out_dim):
#         super().__init__()
#         self.fc = nn.Linear(in_dim, out_dim)
#     def forward(self, x):
#         return F.relu(self.fc(x))

# class CrossAttention(nn.Module):
#     def __init__(self, d_model):
#         super().__init__()
#         self.query = nn.Linear(d_model, d_model)
#         self.key = nn.Linear(d_model, d_model)
#         self.value = nn.Linear(d_model, d_model)
#     def forward(self, audio_emb, landmark_embs):
#         Q = self.query(audio_emb).unsqueeze(1)
#         K = self.key(landmark_embs)
#         V = self.value(landmark_embs)
#         attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (K.size(-1) ** 0.5)
#         attn_weights = F.softmax(attn_scores, dim=-1)
#         out = torch.matmul(attn_weights, V)
#         return out.squeeze(1)

# class TemporalAttention(nn.Module):
#     def __init__(self, hidden_dim):
#         super().__init__()
#         self.attn_fc = nn.Linear(hidden_dim*2, 1)
#     def forward(self, lstm_out, lengths):
#         attn_scores = self.attn_fc(lstm_out).squeeze(-1)
#         mask = torch.arange(lstm_out.size(1))[None, :].to(lengths.device) < lengths[:, None]
#         attn_scores[~mask] = float('-inf')
#         attn_weights = torch.softmax(attn_scores, dim=1).unsqueeze(-1)
#         weighted_sum = (lstm_out * attn_weights).sum(dim=1)
#         return weighted_sum

# class AudioVisualEmotionModel(nn.Module):
#     def __init__(self, node_feat_dim, audio_feat_dim, hidden_dim, num_classes):
#         super().__init__()
#         self.landmark_gat = LandmarkGraphEncoder(node_feat_dim, hidden_dim, hidden_dim)
#         self.audio_encoder = AudioEncoder(audio_feat_dim, hidden_dim)
#         self.cross_attention = CrossAttention(hidden_dim)
#         self.bilstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)
#         self.temporal_attn = TemporalAttention(hidden_dim)
#         self.fc = nn.Linear(hidden_dim*2, num_classes)

#     def forward(self, mfcc_batch, adj, frame_graphs_list, lengths):
#         batch_size = len(mfcc_batch)
#         device = mfcc_batch[0].device
#         hidden_dim = self.audio_encoder.fc.out_features

#         frame_embeddings = []
#         max_seq_len = max(lengths).item()

#         for i in range(batch_size):
#             frames = frame_graphs_list[i]
#             seq_len = len(frames)
#             video_emb = []
#             for t in range(seq_len):
#                 lf = frames[t].x.to(device)
#                 af = mfcc_batch[i][t].unsqueeze(0)
#                 lf_emb = self.landmark_gat(lf, adj)
#                 af_emb = self.audio_encoder(af)
#                 fused = self.cross_attention(af_emb, lf_emb)
#                 video_emb.append(fused)
#             video_emb = torch.cat(video_emb, dim=0)
#             if seq_len < max_seq_len:
#                 pad = torch.zeros(max_seq_len - seq_len, hidden_dim, device=device)
#                 video_emb = torch.cat([video_emb, pad], dim=0)
#             frame_embeddings.append(video_emb)

#         frame_embeddings = torch.stack(frame_embeddings, dim=0)
#         packed = pack_padded_sequence(frame_embeddings, lengths.cpu(), batch_first=True, enforce_sorted=False)
#         lstm_out, _ = self.bilstm(packed)
#         lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True)
#         video_embeds = self.temporal_attn(lstm_out, lengths)
#         out = self.fc(video_embeds)
#         return out

# # ---------------------------
# # 3. Train / Validation Functions
# # ---------------------------
# def train_epoch(model, loader, criterion, optimizer, device):
#     model.train()
#     total_loss = 0.0
#     for mfccs, frame_graphs_list, labels, lengths in loader:
#         labels = labels.to(device)
#         mfccs = [f.to(device) for f in mfccs]
#         adj = frame_graphs_list[0][0].edge_index.to(device)
#         optimizer.zero_grad()
#         outputs = model(mfccs, adj, frame_graphs_list, lengths)
#         loss = criterion(outputs, labels)
#         loss.backward()
#         optimizer.step()
#         total_loss += loss.item() * len(labels)
#     return total_loss / len(loader.dataset)

# def validate_epoch(model, loader, criterion, device):
#     model.eval()
#     total_loss = 0.0
#     all_labels, all_preds = [], []
#     with torch.no_grad():
#         for mfccs, frame_graphs_list, labels, lengths in loader:
#             labels = labels.to(device)
#             mfccs = [f.to(device) for f in mfccs]
#             adj = frame_graphs_list[0][0].edge_index.to(device)
#             outputs = model(mfccs, adj, frame_graphs_list, lengths)
#             loss = criterion(outputs, labels)
#             total_loss += loss.item() * len(labels)
#             preds = torch.argmax(outputs, dim=1)
#             all_labels.extend(labels.cpu().numpy())
#             all_preds.extend(preds.cpu().numpy())
#     acc = sum([p==t for p,t in zip(all_preds, all_labels)]) / len(all_labels)
#     return total_loss / len(loader.dataset), acc, all_labels, all_preds

# # ---------------------------
# # 4. Main Script
# # ---------------------------
# def main():
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#     data_dir = "/content/drive/MyDrive/RAVDESS/Landmarks_WithAudio"
#     labels_csv = "/content/drive/MyDrive/RAVDESS/emotion_labels.csv"

#     full_dataset = RAVDESSDataset(data_dir, labels_csv)

#     train_files, val_files = train_test_split(
#         full_dataset.files, test_size=0.2, stratify=full_dataset.labels_df["emotion_label"], random_state=42
#     )
#     train_idx = [full_dataset.files.index(f) for f in train_files]
#     val_idx = [full_dataset.files.index(f) for f in val_files]

#     train_dataset = Subset(full_dataset, train_idx)
#     val_dataset = Subset(full_dataset, val_idx)

#     train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)
#     val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)

#     model = AudioVisualEmotionModel(node_feat_dim=2, audio_feat_dim=40, hidden_dim=64, num_classes=8)
#     model.to(device)
#     criterion = nn.CrossEntropyLoss()
#     optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

#     best_val_acc = 0.0
#     num_epochs = 50

#     for epoch in range(num_epochs):
#         train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
#         val_loss, val_acc, _, _ = validate_epoch(model, val_loader, criterion, device)
#         print(f"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}")

#         if val_acc > best_val_acc:
#             best_val_acc = val_acc
#             torch.save(model.state_dict(), "best_model.pth")

#     # Confusion matrix
#     _, _, true_labels, pred_labels = validate_epoch(model, val_loader, criterion, device)
#     cm = confusion_matrix(true_labels, pred_labels)
#     disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(full_dataset.emotion_to_id.keys()))
#     disp.plot(cmap=plt.cm.Blues)
#     plt.show()

# if __name__ == "__main__":
#     main()

# ==========================================
# FULL AUDIO-VISUAL EMOTION RECOGNITION SCRIPT (OPTIMIZED GPU VERSION)
# ==========================================

# ---------------------------
# Imports
# ---------------------------
import os
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, Subset
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv, GATConv
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence

# ---------------------------
# 1. Dataset
# ---------------------------
class RAVDESSDataset(Dataset):
    def __init__(self, data_dir, labels_csv):
        self.data_dir = data_dir
        self.labels_df = pd.read_csv(labels_csv)
        self.files = sorted(os.listdir(data_dir))

        self.emotion_to_id = {
            'neutral': 0, 'calm': 1, 'happy': 2, 'sad': 3,
            'angry': 4, 'fearful': 5, 'disgust': 6, 'surprised': 7
        }
        self.labels_df["emotion_label"] = self.labels_df["emotion_label"].map(self.emotion_to_id)
        self.filename_to_label = dict(zip(self.labels_df["filename"], self.labels_df["emotion_label"]))
        self.edge_index = torch.tensor(self.get_anatomical_edge_list(), dtype=torch.long).t().contiguous()

    def __len__(self):
        return len(self.files)

    def get_anatomical_edge_list(self):
        edge_index = []
        for i in range(7): edge_index.append((i, i + 1))
        edge_index.append((7, 0))
        for i in range(8, 19): edge_index.append((i, i + 1))
        edge_index.append((19, 8))
        for i in range(20, 27): edge_index.append((i, i + 1))
        edge_index.append((27, 20))
        for i in range(28, 35): edge_index.append((i, i + 1))
        edge_index.append((35, 28))
        for i in range(36, 47): edge_index.append((i, i + 1))
        edge_index.append((41, 36))
        for i in range(48, 55): edge_index.append((i, i + 1))
        edge_index.append((55, 48))
        for i in range(56, 72): edge_index.append((i, i + 1))
        edge_index.append((72, 56))
        for i in range(73, 77): edge_index.append((i, i + 1))
        edge_index.append((77, 73))
        for i in range(78, 82): edge_index.append((i, i + 1))
        edge_index.append((82, 78))
        for i in range(83, 91): edge_index.append((i, i + 1))
        edge_index.append((91, 83))
        for i in range(104, 115): edge_index.append((i, i + 1))
        edge_index.append((115, 104))
        for i in range(116, 123): edge_index.append((i, i + 1))
        edge_index.append((123, 116))
        edge_index += [(83, 92),(83, 98),(91, 110),(91, 120),(73, 92),(77, 97),
                       (78, 98),(82, 103),(56, 116),(72, 123),(92, 78),(98, 73),(83, 91),(110, 120)]
        return edge_index

    def __getitem__(self, idx):
        filename = self.files[idx]
        file_path = os.path.join(self.data_dir, filename)

        with open(file_path, "rb") as f:
            video_data = pickle.load(f)

        landmarks_all, mfcc_frames = [], []
        for frame in video_data:
            landmarks = np.array(frame['landmarks'], dtype=np.float32)
            landmarks -= landmarks.mean(axis=0)
            mfcc = np.array(frame['mfcc'], dtype=np.float32)
            landmarks_all.append(landmarks)
            mfcc_frames.append(mfcc)

        landmarks_tensor = torch.tensor(np.stack(landmarks_all), dtype=torch.float)
        mfcc_tensor = torch.tensor(np.stack(mfcc_frames), dtype=torch.float)
        mfcc_tensor = (mfcc_tensor - mfcc_tensor.mean(axis=0)) / (mfcc_tensor.std(axis=0) + 1e-6)

        label = torch.tensor(self.filename_to_label[filename], dtype=torch.long)
        seq_len = mfcc_tensor.size(0)
        return mfcc_tensor, landmarks_tensor, label, seq_len, self.edge_index

# ---------------------------
# Collate function
# ---------------------------
def collate_fn(batch):
    mfccs, landmarks, labels, lengths, edges = zip(*batch)
    lengths = torch.tensor(lengths, dtype=torch.long)
    labels = torch.tensor(labels, dtype=torch.long)
    return mfccs, landmarks, labels, lengths, edges[0]

# ---------------------------
# 2. Model
# ---------------------------
class LandmarkGraphEncoder(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, heads=4):
        super().__init__()
        self.gcn = GCNConv(in_dim, hidden_dim)
        self.gat = GATConv(hidden_dim, out_dim, heads=heads, concat=False)

    def forward(self, all_landmarks, edge_index):
        # all_landmarks: (batch * frames, num_nodes, node_feat)
        B, Fr, N, D = all_landmarks.shape
        x = all_landmarks.reshape(B * Fr, N, D).reshape(-1, D)
        # Repeat edge index for each frame
        edge_index = edge_index.repeat(1, B * Fr) + torch.arange(0, B * Fr * N, N, device=edge_index.device).repeat_interleave(edge_index.size(1))
        x = self.gcn(x, edge_index)
        x = F.relu(x)
        x = self.gat(x, edge_index)
        x = x.view(B, Fr, N, -1)
        return x

class AudioEncoder(nn.Module):
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.fc = nn.Linear(in_dim, out_dim)
    def forward(self, x):
        return F.relu(self.fc(x))

class CrossAttention(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.query = nn.Linear(d_model, d_model)
        self.key = nn.Linear(d_model, d_model)
        self.value = nn.Linear(d_model, d_model)
    def forward(self, audio_emb, landmark_embs):
        Q = self.query(audio_emb).unsqueeze(2)
        K = self.key(landmark_embs)
        V = self.value(landmark_embs)
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (K.size(-1) ** 0.5)
        attn_weights = F.softmax(attn_scores, dim=-1)
        out = torch.matmul(attn_weights, V)
        return out.squeeze(2)

class TemporalAttention(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.attn_fc = nn.Linear(hidden_dim*2, 1)
    def forward(self, lstm_out, lengths):
        attn_scores = self.attn_fc(lstm_out).squeeze(-1)
        mask = torch.arange(lstm_out.size(1))[None, :].to(lengths.device) < lengths[:, None]
        attn_scores[~mask] = float('-inf')
        attn_weights = torch.softmax(attn_scores, dim=1).unsqueeze(-1)
        return (lstm_out * attn_weights).sum(dim=1)

class AudioVisualEmotionModel(nn.Module):
    def __init__(self, node_feat_dim, audio_feat_dim, hidden_dim, num_classes):
        super().__init__()
        self.landmark_gat = LandmarkGraphEncoder(node_feat_dim, hidden_dim, hidden_dim)
        self.audio_encoder = AudioEncoder(audio_feat_dim, hidden_dim)
        self.cross_attention = CrossAttention(hidden_dim)
        self.bilstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)
        self.temporal_attn = TemporalAttention(hidden_dim)
        self.fc = nn.Linear(hidden_dim*2, num_classes)

    def forward(self, mfcc_batch, landmarks_batch, edge_index, lengths):
        device = next(self.parameters()).device

        edge_index = edge_index.to(device)
        mfcc_batch = [mfcc.to(device) for mfcc in mfcc_batch]
        landmarks_batch = [lm.to(device) for lm in landmarks_batch]
        lengths = lengths.to(device)

        batch_size = len(mfcc_batch)
        max_seq = max(lengths)
        hidden_dim = self.audio_encoder.fc.out_features

        # Pad MFCC and Landmarks for batching
        padded_mfccs = torch.zeros(batch_size, max_seq, mfcc_batch[0].shape[1], device=device)
        padded_landmarks = torch.zeros(batch_size, max_seq, landmarks_batch[0].shape[1], landmarks_batch[0].shape[2], device=device)
        for i, (mfcc, lm) in enumerate(zip(mfcc_batch, landmarks_batch)):
            padded_mfccs[i, :mfcc.shape[0]] = mfcc
            padded_landmarks[i, :lm.shape[0]] = lm

        with torch.amp.autocast('cuda', enabled=True):
            landmark_embs = self.landmark_gat(padded_landmarks, edge_index)
            audio_embs = self.audio_encoder(padded_mfccs)
            fused = self.cross_attention(audio_embs, landmark_embs)
            packed = pack_padded_sequence(fused, lengths.cpu(), batch_first=True, enforce_sorted=False)
            lstm_out, _ = self.bilstm(packed)
            lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True)
            video_embeds = self.temporal_attn(lstm_out, lengths)
            out = self.fc(video_embeds)
        return out

# ---------------------------
# 3. Train / Validation Functions
# ---------------------------
def train_epoch(model, loader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    for mfccs, landmarks, labels, lengths, edge_index in loader:
        labels = labels.to(device)
        optimizer.zero_grad(set_to_none=True)
        outputs = model(mfccs, landmarks, edge_index, lengths)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * len(labels)
    return total_loss / len(loader.dataset)

def validate_epoch(model, loader, criterion, device):
    model.eval()
    total_loss, all_labels, all_preds = 0, [], []
    with torch.no_grad():
        for mfccs, landmarks, labels, lengths, edge_index in loader:
            labels = labels.to(device)
            outputs = model(mfccs, landmarks, edge_index, lengths)
            loss = criterion(outputs, labels)
            total_loss += loss.item() * len(labels)
            preds = torch.argmax(outputs, dim=1)
            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())
    acc = np.mean(np.array(all_labels) == np.array(all_preds))
    return total_loss / len(loader.dataset), acc, all_labels, all_preds

# ---------------------------
# 4. Main Script
# ---------------------------
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    data_dir = "/content/drive/MyDrive/RAVDESS/Landmarks_WithAudio"
    labels_csv = "/content/drive/MyDrive/RAVDESS/emotion_labels.csv"

    full_dataset = RAVDESSDataset(data_dir, labels_csv)
    train_files, val_files = train_test_split(
        full_dataset.files, test_size=0.2,
        stratify=full_dataset.labels_df["emotion_label"], random_state=42
    )
    train_idx = [full_dataset.files.index(f) for f in train_files]
    val_idx = [full_dataset.files.index(f) for f in val_files]
    train_dataset, val_dataset = Subset(full_dataset, train_idx), Subset(full_dataset, val_idx)

    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)

    model = AudioVisualEmotionModel(node_feat_dim=2, audio_feat_dim=40, hidden_dim=64, num_classes=8).to(device)
    criterion, optimizer = nn.CrossEntropyLoss(), torch.optim.Adam(model.parameters(), lr=1e-4)

    best_val_acc = 0
    for epoch in range(50):
        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
        val_loss, val_acc, _, _ = validate_epoch(model, val_loader, criterion, device)
        print(f"Epoch [{epoch+1}/50] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}")

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), "best_model.pth")

    _, _, true_labels, pred_labels = validate_epoch(model, val_loader, criterion, device)
    cm = confusion_matrix(true_labels, pred_labels)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(full_dataset.emotion_to_id.keys()))
    disp.plot(cmap=plt.cm.Blues)
    plt.show()

if __name__ == "__main__":
    main()

# ==========================================
# FULL OPTIMIZED AUDIO-VISUAL EMOTION RECOGNITION
# ==========================================

# ---------------------------
# Imports
# ---------------------------
import os
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, Subset
from torch_geometric.nn import GCNConv, GATConv
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence

# ---------------------------
# 1. Dataset
# ---------------------------
class RAVDESSDataset(Dataset):
    def __init__(self, data_dir, labels_csv):
        self.data_dir = data_dir
        self.labels_df = pd.read_csv(labels_csv)
        self.files = sorted(os.listdir(data_dir))

        self.emotion_to_id = {
            'neutral': 0, 'calm': 1, 'happy': 2, 'sad': 3,
            'angry': 4, 'fearful': 5, 'disgust': 6, 'surprised': 7
        }
        self.labels_df["emotion_label"] = self.labels_df["emotion_label"].map(self.emotion_to_id)
        self.filename_to_label = dict(zip(self.labels_df["filename"], self.labels_df["emotion_label"]))
        self.edge_index = torch.tensor(self.get_anatomical_edge_list(), dtype=torch.long).t().contiguous()

    def __len__(self):
        return len(self.files)

    def get_anatomical_edge_list(self):
        edge_index = []
        for i in range(7): edge_index.append((i, i + 1))
        edge_index.append((7, 0))
        for i in range(8, 19): edge_index.append((i, i + 1))
        edge_index.append((19, 8))
        for i in range(20, 27): edge_index.append((i, i + 1))
        edge_index.append((27, 20))
        for i in range(28, 35): edge_index.append((i, i + 1))
        edge_index.append((35, 28))
        for i in range(36, 47): edge_index.append((i, i + 1))
        edge_index.append((41, 36))
        for i in range(48, 55): edge_index.append((i, i + 1))
        edge_index.append((55, 48))
        for i in range(56, 72): edge_index.append((i, i + 1))
        edge_index.append((72, 56))
        for i in range(73, 77): edge_index.append((i, i + 1))
        edge_index.append((77, 73))
        for i in range(78, 82): edge_index.append((i, i + 1))
        edge_index.append((82, 78))
        for i in range(83, 91): edge_index.append((i, i + 1))
        edge_index.append((91, 83))
        for i in range(104, 115): edge_index.append((i, i + 1))
        edge_index.append((115, 104))
        for i in range(116, 123): edge_index.append((i, i + 1))
        edge_index.append((123, 116))
        edge_index += [(83, 92),(83, 98),(91, 110),(91, 120),(73, 92),(77, 97),
                       (78, 98),(82, 103),(56, 116),(72, 123),(92, 78),(98, 73),(83, 91),(110, 120)]
        return edge_index

    def __getitem__(self, idx):
        filename = self.files[idx]
        file_path = os.path.join(self.data_dir, filename)
        with open(file_path, "rb") as f:
            video_data = pickle.load(f)

        landmarks_all, mfcc_frames = [], []
        for frame in video_data:
            landmarks = np.array(frame['landmarks'], dtype=np.float32)
            landmarks -= landmarks.mean(axis=0)
            mfcc = np.array(frame['mfcc'], dtype=np.float32)
            landmarks_all.append(landmarks)
            mfcc_frames.append(mfcc)

        landmarks_tensor = torch.tensor(np.stack(landmarks_all), dtype=torch.float)
        mfcc_tensor = torch.tensor(np.stack(mfcc_frames), dtype=torch.float)
        mfcc_tensor = (mfcc_tensor - mfcc_tensor.mean(axis=0)) / (mfcc_tensor.std(axis=0) + 1e-6)

        label = torch.tensor(self.filename_to_label[filename], dtype=torch.long)
        seq_len = mfcc_tensor.size(0)
        return mfcc_tensor, landmarks_tensor, label, seq_len, self.edge_index

# ---------------------------
# Collate function
# ---------------------------
def collate_fn(batch):
    mfccs, landmarks, labels, lengths, edges = zip(*batch)
    lengths = torch.tensor(lengths, dtype=torch.long)
    labels = torch.tensor(labels, dtype=torch.long)
    return mfccs, landmarks, labels, lengths, edges[0]

# ---------------------------
# 2. Model
# ---------------------------
class LandmarkGraphEncoder(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, heads=4, dropout=0.2):
        super().__init__()
        self.gcn = GCNConv(in_dim, hidden_dim)
        self.gat = GATConv(hidden_dim, out_dim, heads=heads, concat=False, dropout=dropout)
        self.dropout = nn.Dropout(dropout)

    def forward(self, all_landmarks, edge_index):
        B, Fr, N, D = all_landmarks.shape
        x = all_landmarks.reshape(B * Fr, N, D).reshape(-1, D)
        edge_index = edge_index.repeat(1, B * Fr) + torch.arange(
            0, B * Fr * N, N, device=edge_index.device
        ).repeat_interleave(edge_index.size(1))
        x = self.gcn(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.gat(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)
        x = x.view(B, Fr, N, -1)
        return x

class AudioEncoder(nn.Module):
    def __init__(self, in_dim, out_dim, dropout=0.2):
        super().__init__()
        self.fc = nn.Linear(in_dim, out_dim)
        self.dropout = nn.Dropout(dropout)
    def forward(self, x):
        x = F.relu(self.fc(x))
        return self.dropout(x)

class CrossAttention(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.query = nn.Linear(d_model, d_model)
        self.key = nn.Linear(d_model, d_model)
        self.value = nn.Linear(d_model, d_model)
    def forward(self, audio_emb, landmark_embs):
        Q = self.query(audio_emb).unsqueeze(2)
        K = self.key(landmark_embs)
        V = self.value(landmark_embs)
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (K.size(-1) ** 0.5)
        attn_weights = F.softmax(attn_scores, dim=-1)
        out = torch.matmul(attn_weights, V)
        return out.squeeze(2)

class TemporalAttention(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.attn_fc = nn.Linear(hidden_dim*2, 1)
    def forward(self, lstm_out, lengths):
        attn_scores = self.attn_fc(lstm_out).squeeze(-1)
        device = lengths.device
        mask = torch.arange(lstm_out.size(1), device=device)[None, :] < lengths[:, None]
        attn_scores[~mask] = -1e9
        attn_weights = torch.softmax(attn_scores, dim=1).unsqueeze(-1)
        return (lstm_out * attn_weights).sum(dim=1)

class AudioVisualEmotionModel(nn.Module):
    def __init__(self, node_feat_dim, audio_feat_dim, hidden_dim, num_classes, dropout=0.2):
        super().__init__()
        self.landmark_gat = LandmarkGraphEncoder(node_feat_dim, hidden_dim, hidden_dim, dropout=dropout)
        self.audio_encoder = AudioEncoder(audio_feat_dim, hidden_dim, dropout=dropout)
        self.cross_attention = CrossAttention(hidden_dim)
        self.bilstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=True, dropout=dropout)
        self.temporal_attn = TemporalAttention(hidden_dim)
        self.fc = nn.Linear(hidden_dim*2, num_classes)

    def forward(self, mfcc_batch, landmarks_batch, edge_index, lengths):
        device = next(self.parameters()).device
        edge_index = edge_index.to(device)
        mfcc_batch = [mfcc.to(device) for mfcc in mfcc_batch]
        landmarks_batch = [lm.to(device) for lm in landmarks_batch]
        lengths = lengths.to(device)

        batch_size = len(mfcc_batch)
        max_seq = max(lengths)
        padded_mfccs = torch.zeros(batch_size, max_seq, mfcc_batch[0].shape[1], device=device)
        padded_landmarks = torch.zeros(batch_size, max_seq, landmarks_batch[0].shape[1], landmarks_batch[0].shape[2], device=device)
        for i, (mfcc, lm) in enumerate(zip(mfcc_batch, landmarks_batch)):
            padded_mfccs[i, :mfcc.shape[0]] = mfcc
            padded_landmarks[i, :lm.shape[0]] = lm

        with torch.amp.autocast(device_type='cuda', enabled=True):
            landmark_embs = self.landmark_gat(padded_landmarks, edge_index)
            audio_embs = self.audio_encoder(padded_mfccs)
            fused = self.cross_attention(audio_embs, landmark_embs)
            packed = pack_padded_sequence(fused, lengths.cpu(), batch_first=True, enforce_sorted=False)
            lstm_out, _ = self.bilstm(packed)
            lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True)
            video_embeds = self.temporal_attn(lstm_out, lengths)
            out = self.fc(video_embeds)
        return out

# ---------------------------
# 3. Train / Validation
# ---------------------------
def train_epoch(model, loader, criterion, optimizer, device, max_grad_norm=1.0):
    model.train()
    total_loss = 0
    for mfccs, landmarks, labels, lengths, edge_index in loader:
        labels = labels.to(device)
        optimizer.zero_grad(set_to_none=True)
        outputs = model(mfccs, landmarks, edge_index, lengths)
        loss = criterion(outputs, labels)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        optimizer.step()
        total_loss += loss.item() * len(labels)
    return total_loss / len(loader.dataset)

def validate_epoch(model, loader, criterion, device):
    model.eval()
    total_loss, all_labels, all_preds = 0, [], []
    with torch.no_grad():
        for mfccs, landmarks, labels, lengths, edge_index in loader:
            labels = labels.to(device)
            outputs = model(mfccs, landmarks, edge_index, lengths)
            loss = criterion(outputs, labels)
            total_loss += loss.item() * len(labels)
            preds = torch.argmax(outputs, dim=1)
            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())
    acc = np.mean(np.array(all_labels) == np.array(all_preds))
    return total_loss / len(loader.dataset), acc, all_labels, all_preds

# ---------------------------
# 4. Main
# ---------------------------
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    data_dir = "/content/drive/MyDrive/RAVDESS/Landmarks_WithAudio"
    labels_csv = "/content/drive/MyDrive/RAVDESS/emotion_labels.csv"

    full_dataset = RAVDESSDataset(data_dir, labels_csv)
    train_files, val_files = train_test_split(
        full_dataset.files, test_size=0.2,
        stratify=full_dataset.labels_df["emotion_label"], random_state=42
    )
    train_idx = [full_dataset.files.index(f) for f in train_files]
    val_idx = [full_dataset.files.index(f) for f in val_files]
    train_dataset, val_dataset = Subset(full_dataset, train_idx), Subset(full_dataset, val_idx)

    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)

    model = AudioVisualEmotionModel(node_feat_dim=2, audio_feat_dim=40, hidden_dim=64, num_classes=8, dropout=0.2).to(device)
    criterion, optimizer = nn.CrossEntropyLoss(), torch.optim.Adam(model.parameters(), lr=1e-4)

    best_val_acc = 0
    for epoch in range(50):
        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
        val_loss, val_acc, _, _ = validate_epoch(model, val_loader, criterion, device)
        print(f"Epoch [{epoch+1}/50] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}")

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), "best_model.pth")

    # Confusion Matrix
    _, _, true_labels, pred_labels = validate_epoch(model, val_loader, criterion, device)
    cm = confusion_matrix(true_labels, pred_labels)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(full_dataset.emotion_to_id.keys()))
    disp.plot(cmap=plt.cm.Blues)
    plt.show()

if __name__ == "__main__":
    main()

# ==========================================
# FULL OPTIMIZED AUDIO-VISUAL EMOTION RECOGNITION
# ==========================================

# ---------------------------
# Imports
# ---------------------------
import os
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, Subset
from torch_geometric.nn import GCNConv, GATConv
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence

# ---------------------------
# 1. Dataset
# ---------------------------
class RAVDESSDataset(Dataset):
    def __init__(self, data_dir, labels_csv):
        self.data_dir = data_dir
        self.labels_df = pd.read_csv(labels_csv)
        self.files = sorted(os.listdir(data_dir))

        self.emotion_to_id = {
            'neutral': 0, 'calm': 1, 'happy': 2, 'sad': 3,
            'angry': 4, 'fearful': 5, 'disgust': 6, 'surprised': 7
        }
        self.labels_df["emotion_label"] = self.labels_df["emotion_label"].map(self.emotion_to_id)
        self.filename_to_label = dict(zip(self.labels_df["filename"], self.labels_df["emotion_label"]))
        self.edge_index = torch.tensor(self.get_anatomical_edge_list(), dtype=torch.long).t().contiguous()

    def __len__(self):
        return len(self.files)

    def get_anatomical_edge_list(self):
        edge_index = []
        for i in range(7): edge_index.append((i, i + 1))
        edge_index.append((7, 0))
        for i in range(8, 19): edge_index.append((i, i + 1))
        edge_index.append((19, 8))
        for i in range(20, 27): edge_index.append((i, i + 1))
        edge_index.append((27, 20))
        for i in range(28, 35): edge_index.append((i, i + 1))
        edge_index.append((35, 28))
        for i in range(36, 47): edge_index.append((i, i + 1))
        edge_index.append((41, 36))
        for i in range(48, 55): edge_index.append((i, i + 1))
        edge_index.append((55, 48))
        for i in range(56, 72): edge_index.append((i, i + 1))
        edge_index.append((72, 56))
        for i in range(73, 77): edge_index.append((i, i + 1))
        edge_index.append((77, 73))
        for i in range(78, 82): edge_index.append((i, i + 1))
        edge_index.append((82, 78))
        for i in range(83, 91): edge_index.append((i, i + 1))
        edge_index.append((91, 83))
        for i in range(104, 115): edge_index.append((i, i + 1))
        edge_index.append((115, 104))
        for i in range(116, 123): edge_index.append((i, i + 1))
        edge_index.append((123, 116))
        edge_index += [(83, 92),(83, 98),(91, 110),(91, 120),(73, 92),(77, 97),
                       (78, 98),(82, 103),(56, 116),(72, 123),(92, 78),(98, 73),(83, 91),(110, 120)]
        return edge_index

    def __getitem__(self, idx):
        filename = self.files[idx]
        file_path = os.path.join(self.data_dir, filename)
        with open(file_path, "rb") as f:
            video_data = pickle.load(f)

        landmarks_all, mfcc_frames = [], []
        for frame in video_data:
            landmarks = np.array(frame['landmarks'], dtype=np.float32)
            landmarks -= landmarks.mean(axis=0)
            mfcc = np.array(frame['mfcc'], dtype=np.float32)
            landmarks_all.append(landmarks)
            mfcc_frames.append(mfcc)

        landmarks_tensor = torch.tensor(np.stack(landmarks_all), dtype=torch.float)
        mfcc_tensor = torch.tensor(np.stack(mfcc_frames), dtype=torch.float)
        mfcc_tensor = (mfcc_tensor - mfcc_tensor.mean(axis=0)) / (mfcc_tensor.std(axis=0) + 1e-6)

        label = torch.tensor(self.filename_to_label[filename], dtype=torch.long)
        seq_len = mfcc_tensor.size(0)
        return mfcc_tensor, landmarks_tensor, label, seq_len, self.edge_index

# ---------------------------
# Collate function
# ---------------------------
def collate_fn(batch):
    mfccs, landmarks, labels, lengths, edges = zip(*batch)
    lengths = torch.tensor(lengths, dtype=torch.long)
    labels = torch.tensor(labels, dtype=torch.long)
    return mfccs, landmarks, labels, lengths, edges[0]

# ---------------------------
# 2. Model
# ---------------------------
class LandmarkGraphEncoder(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, heads=4, dropout=0.2):
        super().__init__()
        self.gcn = GCNConv(in_dim, hidden_dim)
        self.gat = GATConv(hidden_dim, out_dim, heads=heads, concat=False, dropout=dropout)
        self.dropout = nn.Dropout(dropout)

    def forward(self, all_landmarks, edge_index):
        B, Fr, N, D = all_landmarks.shape
        x = all_landmarks.reshape(B * Fr, N, D).reshape(-1, D)
        edge_index = edge_index.repeat(1, B * Fr) + torch.arange(
            0, B * Fr * N, N, device=edge_index.device
        ).repeat_interleave(edge_index.size(1))
        x = self.gcn(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.gat(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)
        x = x.view(B, Fr, N, -1)
        return x

class AudioEncoder(nn.Module):
    def __init__(self, in_dim, out_dim, dropout=0.2):
        super().__init__()
        self.fc = nn.Linear(in_dim, out_dim)
        self.dropout = nn.Dropout(dropout)
    def forward(self, x):
        x = F.relu(self.fc(x))
        return self.dropout(x)

class CrossAttention(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.query = nn.Linear(d_model, d_model)
        self.key = nn.Linear(d_model, d_model)
        self.value = nn.Linear(d_model, d_model)
    def forward(self, audio_emb, landmark_embs):
        Q = self.query(audio_emb).unsqueeze(2)
        K = self.key(landmark_embs)
        V = self.value(landmark_embs)
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (K.size(-1) ** 0.5)
        attn_weights = F.softmax(attn_scores, dim=-1)
        out = torch.matmul(attn_weights, V)
        return out.squeeze(2)

class TemporalAttention(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.attn_fc = nn.Linear(hidden_dim*2, 1)
    def forward(self, lstm_out, lengths):
        attn_scores = self.attn_fc(lstm_out).squeeze(-1)
        device = lengths.device
        mask = torch.arange(lstm_out.size(1), device=device)[None, :] < lengths[:, None]
        attn_scores[~mask] = -1e9
        attn_weights = torch.softmax(attn_scores, dim=1).unsqueeze(-1)
        return (lstm_out * attn_weights).sum(dim=1)

class AudioVisualEmotionModel(nn.Module):
    def __init__(self, node_feat_dim, audio_feat_dim, hidden_dim, num_classes, dropout=0.2):
        super().__init__()
        self.landmark_gat = LandmarkGraphEncoder(node_feat_dim, hidden_dim, hidden_dim, dropout=dropout)
        self.audio_encoder = AudioEncoder(audio_feat_dim, hidden_dim, dropout=dropout)
        self.cross_attention = CrossAttention(hidden_dim)
        self.bilstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=True, dropout=dropout)
        self.temporal_attn = TemporalAttention(hidden_dim)
        self.fc = nn.Linear(hidden_dim*2, num_classes)

    def forward(self, mfcc_batch, landmarks_batch, edge_index, lengths):
        device = next(self.parameters()).device
        edge_index = edge_index.to(device)
        mfcc_batch = [mfcc.to(device) for mfcc in mfcc_batch]
        landmarks_batch = [lm.to(device) for lm in landmarks_batch]
        lengths = lengths.to(device)

        batch_size = len(mfcc_batch)
        max_seq = max(lengths)
        padded_mfccs = torch.zeros(batch_size, max_seq, mfcc_batch[0].shape[1], device=device)
        padded_landmarks = torch.zeros(batch_size, max_seq, landmarks_batch[0].shape[1], landmarks_batch[0].shape[2], device=device)
        for i, (mfcc, lm) in enumerate(zip(mfcc_batch, landmarks_batch)):
            padded_mfccs[i, :mfcc.shape[0]] = mfcc
            padded_landmarks[i, :lm.shape[0]] = lm

        with torch.amp.autocast(device_type='cuda', enabled=True):
            landmark_embs = self.landmark_gat(padded_landmarks, edge_index)
            audio_embs = self.audio_encoder(padded_mfccs)
            fused = self.cross_attention(audio_embs, landmark_embs)
            packed = pack_padded_sequence(fused, lengths.cpu(), batch_first=True, enforce_sorted=False)
            lstm_out, _ = self.bilstm(packed)
            lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True)
            video_embeds = self.temporal_attn(lstm_out, lengths)
            out = self.fc(video_embeds)
        return out

# ---------------------------
# 3. Train / Validation
# ---------------------------
def train_epoch(model, loader, criterion, optimizer, device, max_grad_norm=1.0):
    model.train()
    total_loss = 0
    for mfccs, landmarks, labels, lengths, edge_index in loader:
        labels = labels.to(device)
        optimizer.zero_grad(set_to_none=True)
        outputs = model(mfccs, landmarks, edge_index, lengths)
        loss = criterion(outputs, labels)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        optimizer.step()
        total_loss += loss.item() * len(labels)
    return total_loss / len(loader.dataset)

def validate_epoch(model, loader, criterion, device):
    model.eval()
    total_loss, all_labels, all_preds = 0, [], []
    with torch.no_grad():
        for mfccs, landmarks, labels, lengths, edge_index in loader:
            labels = labels.to(device)
            outputs = model(mfccs, landmarks, edge_index, lengths)
            loss = criterion(outputs, labels)
            total_loss += loss.item() * len(labels)
            preds = torch.argmax(outputs, dim=1)
            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())
    acc = np.mean(np.array(all_labels) == np.array(all_preds))
    return total_loss / len(loader.dataset), acc, all_labels, all_preds

# ---------------------------
# 4. Main
# ---------------------------
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    data_dir = "/content/drive/MyDrive/RAVDESS/Landmarks_WithAudio"
    labels_csv = "/content/drive/MyDrive/RAVDESS/emotion_labels.csv"

    full_dataset = RAVDESSDataset(data_dir, labels_csv)
    train_files, val_files = train_test_split(
        full_dataset.files, test_size=0.2,
        stratify=full_dataset.labels_df["emotion_label"], random_state=42
    )
    train_idx = [full_dataset.files.index(f) for f in train_files]
    val_idx = [full_dataset.files.index(f) for f in val_files]
    train_dataset, val_dataset = Subset(full_dataset, train_idx), Subset(full_dataset, val_idx)

    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)

    model = AudioVisualEmotionModel(node_feat_dim=2, audio_feat_dim=40, hidden_dim=128, num_classes=8, dropout=0.2).to(device)
    criterion, optimizer = nn.CrossEntropyLoss(), torch.optim.Adam(model.parameters(), lr=1e-4)

    best_val_acc = 0
    for epoch in range(50):
        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
        val_loss, val_acc, _, _ = validate_epoch(model, val_loader, criterion, device)
        print(f"Epoch [{epoch+1}/50] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}")

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), "best_model.pth")

    # Confusion Matrix
    _, _, true_labels, pred_labels = validate_epoch(model, val_loader, criterion, device)
    cm = confusion_matrix(true_labels, pred_labels)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(full_dataset.emotion_to_id.keys()))
    disp.plot(cmap=plt.cm.Blues)
    plt.show()

if __name__ == "__main__":
    main()