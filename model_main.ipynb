{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62DHEJkgZFOj",
        "outputId": "1bd0da0a-999e-4b52-9cb4-bf1e219ff612"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCLyKN-BbtnZ",
        "outputId": "a3120729-4a0a-46fe-89f1-c7b5808552f8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0+cu126\n",
            "12.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch_geometric\n",
        "print(torch.__version__)\n",
        "print(torch_geometric.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGKkJC-mZR90",
        "outputId": "1efd1199-ebe1-44aa-bbee-8c56cf0adf43"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0+cu126\n",
            "2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeRe37zHZmh3",
        "outputId": "6eb4c84a-62ae-44b7-f3ea-170e3c005ef1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WnFX7i3x7T3B"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.nn import GCNConv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Graph Attention for Landmarks ----------\n",
        "class LandmarkGraphEncoder(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, heads=4):\n",
        "        super().__init__()\n",
        "        self.gcn = GCNConv(in_dim, hidden_dim)\n",
        "        self.gat = GATConv(hidden_dim, out_dim, heads=heads, concat=False)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.gcn(x, edge_index)       # GCN feature propagation\n",
        "        x = F.relu(x)\n",
        "        x = self.gat(x, edge_index)       # GAT learns attention across landmarks\n",
        "        return x\n",
        "\n",
        "\n",
        "# ---------- Audio Encoder ----------\n",
        "class AudioEncoder(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(in_dim, out_dim)\n",
        "    def forward(self, x):\n",
        "        return F.relu(self.fc(x))  # [batch, out_dim]\n",
        "\n",
        "# ---------- Cross Attention ----------\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "    def forward(self, audio_emb, landmark_embs):\n",
        "        # audio_emb: [batch, d_model]\n",
        "        # landmark_embs: [batch, num_nodes, d_model]\n",
        "        Q = self.query(audio_emb).unsqueeze(1)  # [batch, 1, d_model]\n",
        "        K = self.key(landmark_embs)             # [batch, num_nodes, d_model]\n",
        "        V = self.value(landmark_embs)           # [batch, num_nodes, d_model]\n",
        "\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (K.size(-1) ** 0.5)\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)   # [batch, 1, num_nodes]\n",
        "        out = torch.matmul(attn_weights, V)             # [batch, 1, d_model]\n",
        "        return out.squeeze(1)  # [batch, d_model]"
      ],
      "metadata": {
        "id": "8pGA00iu7Wok"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Final Model ----------\n",
        "class AudioVisualEmotionModel(nn.Module):\n",
        "    def __init__(self, node_feat_dim, audio_feat_dim, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.landmark_gat = LandmarkGraphEncoder(node_feat_dim, hidden_dim, hidden_dim)\n",
        "        self.audio_encoder = AudioEncoder(audio_feat_dim, hidden_dim)\n",
        "        self.cross_attention = CrossAttention(hidden_dim)\n",
        "        self.bilstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim*2, num_classes)\n",
        "\n",
        "    def forward(self, landmark_feats, adj, audio_feats):\n",
        "        \"\"\"\n",
        "        landmark_feats: [batch, num_frames, num_nodes, node_feat_dim]\n",
        "        adj: [num_nodes, num_nodes]\n",
        "        audio_feats: [batch, num_frames, audio_feat_dim]\n",
        "        \"\"\"\n",
        "        batch, num_frames, num_nodes, _ = landmark_feats.size()\n",
        "\n",
        "        frame_embeddings = []\n",
        "        for t in range(num_frames):\n",
        "            lf = landmark_feats[:, t]          # [batch, num_nodes, node_feat_dim]\n",
        "            af = audio_feats[:, t]             # [batch, audio_feat_dim]\n",
        "\n",
        "            lf = self.landmark_gat(lf, adj)    # [batch, num_nodes, hidden_dim]\n",
        "            af = self.audio_encoder(af)        # [batch, hidden_dim]\n",
        "\n",
        "            fused = self.cross_attention(af, lf)  # [batch, hidden_dim]\n",
        "            frame_embeddings.append(fused)\n",
        "\n",
        "        frame_embeddings = torch.stack(frame_embeddings, dim=1)  # [batch, num_frames, hidden_dim]\n",
        "\n",
        "        lstm_out, _ = self.bilstm(frame_embeddings)  # [batch, num_frames, hidden_dim*2]\n",
        "        pooled = torch.mean(lstm_out, dim=1)        # [batch, hidden_dim*2]\n",
        "\n",
        "        out = self.fc(pooled)  # [batch, num_classes]\n",
        "        return out"
      ],
      "metadata": {
        "id": "-HMOV7_wIEDB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from torch_geometric.data import Data, Batch  # PyG for graphs\n",
        "# import pickle\n",
        "# import os\n",
        "\n",
        "# # ---------------------------\n",
        "# # 1. Dataset\n",
        "# # ---------------------------\n",
        "# class RAVDESSDataset(Dataset):\n",
        "#     def __init__(self, data_dir, labels):\n",
        "#         \"\"\"\n",
        "#         mfcc_dir: folder containing per-video MFCC tensors\n",
        "#         landmark_dir: folder containing pickled landmark frames per video\n",
        "#         labels: list of integers (emotion classes)\n",
        "#         \"\"\"\n",
        "#         self.data_dir = data_dir\n",
        "#         self.transform = transform\n",
        "#         self.labels = pd.read_csv(labels_csv)\n",
        "\n",
        "#         # --- Map emotions to numeric labels ---\n",
        "#         self.emotion_to_id = {\n",
        "#             'neutral': 0,\n",
        "#             'calm': 1,\n",
        "#             'happy': 2,\n",
        "#             'sad': 3,\n",
        "#             'angry': 4,\n",
        "#             'fearful': 5,\n",
        "#             'disgust': 6,\n",
        "#             'surprised': 7\n",
        "#         }\n",
        "\n",
        "#         # Apply numeric mapping\n",
        "#         self.labels[\"emotion_label\"] = self.labels[\"emotion_label\"].map(self.emotion_to_id)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.mfcc_files)\n",
        "\n",
        "\n",
        "#     def get_anatomical_edge_list():\n",
        "#       \"\"\"\n",
        "#       Creates edge connections for 68 facial landmarks following anatomical structure.\n",
        "#       Returns list of tuples (source, destination) representing edges.\n",
        "#       \"\"\"\n",
        "#       edge_index = []\n",
        "\n",
        "#       # right iris circumference (8 points: 0-7) - sequential connections\n",
        "#       for i in range(7):\n",
        "#           edge_index.append((i, i + 1))\n",
        "#       edge_index.append((7, 0))\n",
        "\n",
        "#       # right eye boundary (12 points: 8-19) - sequential connections\n",
        "#       for i in range(8, 19):\n",
        "#           edge_index.append((i, i + 1))\n",
        "#       edge_index.append((19, 8))\n",
        "\n",
        "#       # right pupil circumference (8 points: 20-27) - sequential connections\n",
        "#       for i in range(20, 27):\n",
        "#           edge_index.append((i, i + 1))\n",
        "#       edge_index.append((27, 20))\n",
        "\n",
        "#       # left iris circumference (8 points: 28-35) - sequential connections\n",
        "#       for i in range(28, 35):\n",
        "#           edge_index.append((i, i + 1))\n",
        "#       edge_index.append((35, 28))\n",
        "\n",
        "#       # left eye boundary (12 points: 36-47) - form a closed loop\n",
        "#       for i in range(36, 47):\n",
        "#           edge_index.append((i, i + 1))\n",
        "#       edge_index.append((41, 36))  # Close the loop\n",
        "\n",
        "#       # Left pupil circumference (8 points: 48-55) - form a closed loop\n",
        "#       for i in range(48, 55):\n",
        "#           edge_index.append((i, i + 1))\n",
        "#       edge_index.append((55, 48))  # Close the loop\n",
        "\n",
        "#       # jawline (17 points: 56-72) - form a closed loop\n",
        "#       for i in range(56, 72):\n",
        "#           edge_index.append((i, i + 1))\n",
        "#       edge_index.append((72, 56))  # Close the loop\n",
        "\n",
        "#       # right eyebrow (5 points: 73-77) - form a closed loop\n",
        "#       for i in range(73, 77):\n",
        "#           edge_index.append((i, i + 1))\n",
        "#       edge_index.append((77, 73))  # Close the loop\n",
        "\n",
        "#       # left eyebrow (points: 78-82)\n",
        "#       for i in range(78, 82):\n",
        "#           edge_index.append((i, i + 1))\n",
        "#       edge_index.append((82, 78))  # Close the loop\n",
        "\n",
        "#       # nose bridge(83-86) + lower nose (87-91)\n",
        "#       for i in range(83, 91):\n",
        "#           edge_index.append((i, i + 1))\n",
        "#       edge_index.append((91, 83))  # Close the loop\n",
        "\n",
        "#       # right eye boundary 68 dlib marks(92-97)\n",
        "#       for i in range(92, 97):\n",
        "#           edge_index.append((i, i + 1))\n",
        "#       edge_index.append((97, 92))  # Close the loop\n",
        "\n",
        "#       # left eye boundary 68 dlib marks(98-103)\n",
        "#       for i in range(98, 103):\n",
        "#           edge_index.append((i, i + 1))\n",
        "#       edge_index.append((103, 98))  # Close the loop\n",
        "\n",
        "#       # inner mouth (104-115)\n",
        "#       for i in range(104, 115):\n",
        "#           edge_index.append((i, i + 1))\n",
        "#       edge_index.append((115, 104))  # Close the loop\n",
        "\n",
        "#       # outer mouth (116-123)\n",
        "#       for i in range(116, 123):\n",
        "#           edge_index.append((i, i + 1))\n",
        "#       edge_index.append((123, 116))\n",
        "\n",
        "#       #----------------YET TO DECIDE----------------\n",
        "#       # Inter-regional connections (connect major facial regions)\n",
        "#       # edge_index.append((30, 48))  # Nose tip to mouth center\n",
        "#       # edge_index.append((27, 39))  # Nose bridge to right eye\n",
        "#       # edge_index.append((27, 42))  # Nose bridge to left eye\n",
        "#       # edge_index.append((21, 39))  # Right eyebrow to right eye\n",
        "#       # edge_index.append((22, 42))  # Left eyebrow to left eye\n",
        "#       # ---------------- INTER-REGIONAL CONNECTIONS ----------------\n",
        "\n",
        "#       # Nose bridge to eyes\n",
        "#       edge_index += [\n",
        "#           (83, 92),  # upper nose bridge to right eye inner corner\n",
        "#           (83, 98),  # upper nose bridge to left eye inner corner\n",
        "#       ]\n",
        "\n",
        "#       # Nose tip to mouth region\n",
        "#       edge_index += [\n",
        "#           (91, 110),  # nose tip to upper lip center\n",
        "#           (91, 120),  # nose tip to lower lip center\n",
        "#       ]\n",
        "\n",
        "#       # Eyebrows to eyes\n",
        "#       edge_index += [\n",
        "#           (73, 92),  # right eyebrow to right eye outer corner\n",
        "#           (77, 97),  # right eyebrow inner end to right eye inner corner\n",
        "#           (78, 98),  # left eyebrow inner end to left eye inner corner\n",
        "#           (82, 103), # left eyebrow outer end to left eye outer corner\n",
        "#       ]\n",
        "\n",
        "#       # Jawline to mouth corners\n",
        "#       edge_index += [\n",
        "#           (56, 116),  # right jaw to right mouth corner\n",
        "#           (72, 123),  # left jaw to left mouth corner\n",
        "#       ]\n",
        "\n",
        "#       # Cross connections for symmetry (eyes ↔ opposite eyebrows)\n",
        "#       edge_index += [\n",
        "#           (92, 78),   # right eye to left eyebrow\n",
        "#           (98, 73),   # left eye to right eyebrow\n",
        "#       ]\n",
        "\n",
        "#       # Optional central connections for stability\n",
        "#       edge_index += [\n",
        "#           (83, 91),   # along the nose (bridge to tip)\n",
        "#           (110, 120), # upper to lower lip\n",
        "#       ]\n",
        "\n",
        "\n",
        "#       return edge_index\n",
        "\n",
        "# # # Test the function\n",
        "# # anatomical_edges = get_anatomical_edge_list()\n",
        "# # print(f\"Total number of edges: {len(anatomical_edges)}\")\n",
        "# # print(\"First 10 edges:\", anatomical_edges[:10])\n",
        "# # print(\"Last 5 edges:\", anatomical_edges[-5:])\n",
        "\n",
        "# # # Verify all indices are within valid range (0-67)\n",
        "# # max_index = max(max(edge) for edge in anatomical_edges)\n",
        "# # min_index = min(min(edge) for edge in anatomical_edges)\n",
        "# # print(f\"Index range: {min_index} to {max_index} (should be 0 to 67)\")\n",
        "\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         file_path = os.path.join(self.data_dir, self.files[idx])               # [num_frames, mfcc_dim]\n",
        "\n",
        "#         # Load landmark frames (list of [num_nodes, 2])\n",
        "#         with open(file_path, \"rb\") as f:\n",
        "#             video_data = pickle.load(f)                 # list of frames\n",
        "\n",
        "#         # Convert to PyG Data objects\n",
        "#         frame_graphs,mfcc_frames = [],[]\n",
        "#         edge_index = get_anatomical_edge_list()  # your edge connection logic here\n",
        "#         edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
        "#         for frame in video_data:\n",
        "#             landmarks = np.array(frame['landmarks'], dtype=np.float32)\n",
        "#             mfcc = np.array(frame['mfcc'], dtype=np.float32)\n",
        "\n",
        "#             x = torch.tensor(landmarks, dtype=torch.float)\n",
        "#             # edge_index = get_anatomical_edge_list()  # your edge connection logic here\n",
        "#             # edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
        "\n",
        "#             data = Data(x=x, edge_index=edge_index)\n",
        "#             frame_graphs.append(data)\n",
        "#             mfcc_frames.append(mfcc)\n",
        "\n",
        "#         mfcc_tensor = torch.tensor(np.stack(mfcc_frames), dtype=torch.float)  # [num_frames, 40]\n",
        "#         label = torch.tensor(self.emotion_to_idx[self.emotions[idx]], dtype=torch.long)\n",
        "#         return mfcc_tensor, frame_graphs, label\n",
        "\n",
        "# # ---------------------------\n",
        "# # 2. Training / Validation loops\n",
        "# # ---------------------------\n",
        "# def train_epoch(model, loader, criterion, optimizer, device):\n",
        "#     model.train()\n",
        "#     total_loss = 0.0\n",
        "#     for mfcc_batch, landmark_batch_list, labels in loader:\n",
        "#         labels = labels.to(device)\n",
        "#         # mfcc_batch: list of tensors with shape [num_frames, mfcc_dim]\n",
        "#         # landmark_batch_list: list of lists of PyG Data objects per video\n",
        "\n",
        "#         batch_size = len(mfcc_batch)\n",
        "#         num_frames = mfcc_batch[0].size(0)\n",
        "\n",
        "#         # Stack MFCC into [batch, num_frames, mfcc_dim]\n",
        "#         mfcc_batch = torch.stack([torch.tensor(f, dtype=torch.float) for f in mfcc_batch]).to(device)\n",
        "\n",
        "#         # Forward pass\n",
        "#         outputs = []\n",
        "#         for i in range(batch_size):\n",
        "#             video_landmarks = landmark_batch_list[i]       # list of Data objects\n",
        "#             adj = None                                     # using edge_index inside PyG Data\n",
        "#             video_output = model(mfcc_batch[i].unsqueeze(0), adj, video_landmarks)\n",
        "#             outputs.append(video_output)\n",
        "\n",
        "#         outputs = torch.cat(outputs, dim=0)\n",
        "#         loss = criterion(outputs, labels)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item() * batch_size\n",
        "\n",
        "#     return total_loss / len(loader.dataset)\n",
        "\n",
        "# def validate_epoch(model, loader, criterion, device):\n",
        "#     model.eval()\n",
        "#     total_loss = 0.0\n",
        "#     correct = 0\n",
        "#     with torch.no_grad():\n",
        "#         for mfcc_batch, landmark_batch_list, labels in loader:\n",
        "#             labels = labels.to(device)\n",
        "#             batch_size = len(mfcc_batch)\n",
        "#             num_frames = mfcc_batch[0].size(0)\n",
        "\n",
        "#             mfcc_batch = torch.stack([torch.tensor(f, dtype=torch.float) for f in mfcc_batch]).to(device)\n",
        "\n",
        "#             outputs = []\n",
        "#             for i in range(batch_size):\n",
        "#                 video_landmarks = landmark_batch_list[i]\n",
        "#                 adj = None\n",
        "#                 video_output = model(mfcc_batch[i].unsqueeze(0), adj, video_landmarks)\n",
        "#                 outputs.append(video_output)\n",
        "\n",
        "#             outputs = torch.cat(outputs, dim=0)\n",
        "#             total_loss += criterion(outputs, labels).item() * batch_size\n",
        "#             _, predicted = torch.max(outputs, 1)\n",
        "#             correct += (predicted == labels).sum().item()\n",
        "\n",
        "#     accuracy = correct / len(loader.dataset)\n",
        "#     return total_loss / len(loader.dataset), accuracy\n",
        "\n",
        "# # ---------------------------\n",
        "# # 3. Main training script\n",
        "# # ---------------------------\n",
        "# def main():\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#     # Paths to your preprocessed MFCC and landmark data\n",
        "#     data_dir = \"/content/drive/MyDrive/RAVDESS/Landmarks_WithAudio\"\n",
        "#     labels_df = pd.read_csv(\"/content/drive/MyDrive/RAVDESS/emotion_labels.csv\")\n",
        "\n",
        "#     # Labels: list of integers per video\n",
        "#     labels = [...]  # fill in your labels\n",
        "\n",
        "#     # Dataset / Dataloader\n",
        "#     dataset = RAVDESSDataset(mfcc_dir, landmark_dir, labels)\n",
        "#     train_size = int(0.8 * len(dataset))\n",
        "#     val_size = len(dataset) - train_size\n",
        "#     train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "#     train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "#     val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
        "\n",
        "#     # Model\n",
        "#     from your_model_file import AudioVisualEmotionModel, LandmarkGraphEncoder, AudioEncoder, CrossAttention\n",
        "#     model = AudioVisualEmotionModel(node_feat_dim=2, audio_feat_dim=40, hidden_dim=128, num_classes=8)\n",
        "#     model.to(device)\n",
        "\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "#     num_epochs = 50\n",
        "#     best_val_acc = 0.0\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "#         val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
        "#         print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f} \"\n",
        "#               f\"| Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "#         if val_acc > best_val_acc:\n",
        "#             best_val_acc = val_acc\n",
        "#             torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ],
      "metadata": {
        "id": "rL9nHDRlMUbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import pickle\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from torch_geometric.data import Data\n",
        "\n",
        "# # ---------------------------\n",
        "# # 1. Dataset\n",
        "# # ---------------------------\n",
        "# class RAVDESSDataset(Dataset):\n",
        "#     def __init__(self, data_dir, labels_csv):\n",
        "#         \"\"\"\n",
        "#         data_dir: folder containing per-video pickled data with 'landmarks' and 'mfcc'\n",
        "#         labels_csv: CSV file mapping filename -> emotion_label\n",
        "#         \"\"\"\n",
        "#         self.data_dir = data_dir\n",
        "#         self.labels_df = pd.read_csv(labels_csv)\n",
        "#         self.files = sorted(os.listdir(data_dir))\n",
        "\n",
        "#         # Map emotion to numeric labels\n",
        "#         self.emotion_to_id = {\n",
        "#             'neutral': 0,\n",
        "#             'calm': 1,\n",
        "#             'happy': 2,\n",
        "#             'sad': 3,\n",
        "#             'angry': 4,\n",
        "#             'fearful': 5,\n",
        "#             'disgust': 6,\n",
        "#             'surprised': 7\n",
        "#         }\n",
        "\n",
        "#         self.labels_df[\"emotion_label\"] = self.labels_df[\"emotion_label\"].map(self.emotion_to_id)\n",
        "#         self.filename_to_label = dict(zip(self.labels_df[\"filename\"], self.labels_df[\"emotion_label\"]))\n",
        "\n",
        "#         # Precompute fixed anatomical edge index\n",
        "#         self.edge_index = torch.tensor(self.get_anatomical_edge_list(), dtype=torch.long)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.files)\n",
        "\n",
        "#     def get_anatomical_edge_list(self):\n",
        "#         edge_index = []\n",
        "\n",
        "#         # Right iris\n",
        "#         for i in range(7): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((7, 0))\n",
        "\n",
        "#         # Right eye boundary\n",
        "#         for i in range(8, 19): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((19, 8))\n",
        "\n",
        "#         # Right pupil\n",
        "#         for i in range(20, 27): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((27, 20))\n",
        "\n",
        "#         # Left iris\n",
        "#         for i in range(28, 35): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((35, 28))\n",
        "\n",
        "#         # Left eye boundary\n",
        "#         for i in range(36, 47): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((41, 36))\n",
        "\n",
        "#         # Left pupil\n",
        "#         for i in range(48, 55): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((55, 48))\n",
        "\n",
        "#         # Jawline\n",
        "#         for i in range(56, 72): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((72, 56))\n",
        "\n",
        "#         # Right eyebrow\n",
        "#         for i in range(73, 77): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((77, 73))\n",
        "\n",
        "#         # Left eyebrow\n",
        "#         for i in range(78, 82): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((82, 78))\n",
        "\n",
        "#         # Nose bridge + lower nose\n",
        "#         for i in range(83, 91): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((91, 83))\n",
        "\n",
        "#         # Inner mouth\n",
        "#         for i in range(104, 115): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((115, 104))\n",
        "\n",
        "#         # Outer mouth\n",
        "#         for i in range(116, 123): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((123, 116))\n",
        "\n",
        "#         # Additional anatomical connections\n",
        "#         edge_index += [(83, 92),(83, 98),(91, 110),(91, 120),(73, 92),(77, 97),(78, 98),(82, 103),\n",
        "#                        (56, 116),(72, 123),(92, 78),(98, 73),(83, 91),(110, 120)]\n",
        "#         return edge_index\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         filename = self.files[idx]\n",
        "#         file_path = os.path.join(self.data_dir, filename)\n",
        "\n",
        "#         with open(file_path, \"rb\") as f:\n",
        "#             video_data = pickle.load(f)\n",
        "\n",
        "#         frame_graphs = []\n",
        "#         mfcc_frames = []\n",
        "\n",
        "#         for frame in video_data:\n",
        "#             landmarks = np.array(frame['landmarks'], dtype=np.float32)\n",
        "#             # --- Center landmarks per frame ---\n",
        "#             landmarks -= landmarks.mean(axis=0)\n",
        "\n",
        "#             mfcc = np.array(frame['mfcc'], dtype=np.float32)\n",
        "#             mfcc_frames.append(mfcc)\n",
        "\n",
        "#             x = torch.tensor(landmarks, dtype=torch.float)\n",
        "#             data = Data(x=x, edge_index=self.edge_index)\n",
        "#             frame_graphs.append(data)\n",
        "\n",
        "#         # --- Normalize MFCC per video ---\n",
        "#         mfcc_tensor = torch.tensor(np.stack(mfcc_frames), dtype=torch.float)\n",
        "#         mfcc_tensor = (mfcc_tensor - mfcc_tensor.mean(axis=0)) / (mfcc_tensor.std(axis=0) + 1e-6)\n",
        "\n",
        "#         label = torch.tensor(self.filename_to_label[filename], dtype=torch.long)\n",
        "#         return mfcc_tensor, frame_graphs, label\n",
        "\n",
        "# # ---------------------------\n",
        "# # 2. Stratified split helper\n",
        "# # ---------------------------\n",
        "# def stratified_split(labels_csv, test_size=0.2, random_state=42):\n",
        "#     df = pd.read_csv(labels_csv)\n",
        "#     train_files, val_files = train_test_split(\n",
        "#         df['filename'],\n",
        "#         test_size=test_size,\n",
        "#         stratify=df['emotion_label'],\n",
        "#         random_state=random_state\n",
        "#     )\n",
        "#     return train_files.tolist(), val_files.tolist()\n",
        "\n",
        "# # ---------------------------\n",
        "# # 3. Training / Validation loops (unchanged)\n",
        "# # ---------------------------\n",
        "# def train_epoch(model, loader, criterion, optimizer, device):\n",
        "#     model.train()\n",
        "#     total_loss = 0.0\n",
        "#     for mfcc_batch, landmark_batch_list, labels in loader:\n",
        "#         labels = labels.to(device)\n",
        "#         batch_size = len(mfcc_batch)\n",
        "#         mfcc_batch = torch.stack([torch.tensor(f, dtype=torch.float) for f in mfcc_batch]).to(device)\n",
        "\n",
        "#         outputs = []\n",
        "#         for i in range(batch_size):\n",
        "#             video_landmarks = landmark_batch_list[i]\n",
        "#             adj = None\n",
        "#             video_output = model(mfcc_batch[i].unsqueeze(0), adj, video_landmarks)\n",
        "#             outputs.append(video_output)\n",
        "\n",
        "#         outputs = torch.cat(outputs, dim=0)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item() * batch_size\n",
        "\n",
        "#     return total_loss / len(loader.dataset)\n",
        "\n",
        "# def validate_epoch(model, loader, criterion, device):\n",
        "#     model.eval()\n",
        "#     total_loss = 0.0\n",
        "#     all_labels = []\n",
        "#     all_preds = []\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for mfcc_batch, landmark_batch_list, labels in loader:\n",
        "#             labels = labels.to(device)\n",
        "#             batch_size = len(mfcc_batch)\n",
        "\n",
        "#             mfcc_batch = torch.stack([torch.tensor(f, dtype=torch.float) for f in mfcc_batch]).to(device)\n",
        "\n",
        "#             outputs = []\n",
        "#             for i in range(batch_size):\n",
        "#                 video_landmarks = landmark_batch_list[i]\n",
        "#                 adj = None\n",
        "#                 video_output = model(mfcc_batch[i].unsqueeze(0), adj, video_landmarks)\n",
        "#                 outputs.append(video_output)\n",
        "\n",
        "#             outputs = torch.cat(outputs, dim=0)\n",
        "#             total_loss += criterion(outputs, labels).item() * batch_size\n",
        "#             _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "#             all_labels.extend(labels.cpu().numpy())\n",
        "#             all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "#     accuracy = sum([p==t for p,t in zip(all_preds, all_labels)]) / len(all_labels)\n",
        "#     return total_loss / len(loader.dataset), accuracy, all_labels, all_preds\n",
        "\n",
        "\n",
        "# # ---------------------------\n",
        "# # 4. Main training script\n",
        "# # ---------------------------\n",
        "# def main():\n",
        "#     import torch.optim as optim\n",
        "#     from your_model_file import AudioVisualEmotionModel  # replace with your actual import\n",
        "\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     data_dir = \"/content/drive/MyDrive/RAVDESS/Landmarks_WithAudio\"\n",
        "#     labels_csv = \"/content/drive/MyDrive/RAVDESS/emotion_labels.csv\"\n",
        "\n",
        "#     # Stratified split\n",
        "#     train_files, val_files = stratified_split(labels_csv)\n",
        "\n",
        "#     # Custom dataset filtering by split\n",
        "#     full_dataset = RAVDESSDataset(data_dir, labels_csv)\n",
        "#     train_dataset = torch.utils.data.Subset(full_dataset, [full_dataset.files.index(f) for f in train_files])\n",
        "#     val_dataset = torch.utils.data.Subset(full_dataset, [full_dataset.files.index(f) for f in val_files])\n",
        "\n",
        "#     train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "#     val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
        "\n",
        "#     model = AudioVisualEmotionModel(node_feat_dim=2, audio_feat_dim=40, hidden_dim=128, num_classes=8)\n",
        "#     model.to(device)\n",
        "\n",
        "#     criterion = torch.nn.CrossEntropyLoss()\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "#     num_epochs = 50\n",
        "#     best_val_acc = 0.0\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "#         val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
        "#         print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "#         # During your main loop after validation:\n",
        "#         val_loss, val_acc, true_labels, pred_labels = validate_epoch(model, val_loader, criterion, device)\n",
        "#         print(f\"Val Loss: {val_loss:.4f} | Val Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "#         # Confusion matrix\n",
        "#         cm = confusion_matrix(true_labels, pred_labels)\n",
        "#         disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(dataset.emotion_to_id.keys()))\n",
        "#         disp.plot(cmap=plt.cm.Blues)\n",
        "#         plt.show()\n",
        "\n",
        "\n",
        "#         if val_acc > best_val_acc:\n",
        "#             best_val_acc = val_acc\n",
        "#             torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ],
      "metadata": {
        "id": "GlEs_O93V9W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ---------------------------\n",
        "# # Imports\n",
        "# # ---------------------------\n",
        "# import os\n",
        "# import pickle\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from torch.utils.data import Dataset, DataLoader, Subset\n",
        "# from torch_geometric.data import Data\n",
        "# from torch_geometric.nn import GCNConv, GATConv\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "# from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
        "\n",
        "# # ---------------------------\n",
        "# # 1. Dataset\n",
        "# # ---------------------------\n",
        "# class RAVDESSDataset(Dataset):\n",
        "#     def __init__(self, data_dir, labels_csv):\n",
        "#         self.data_dir = data_dir\n",
        "#         self.labels_df = pd.read_csv(labels_csv)\n",
        "#         self.files = sorted(os.listdir(data_dir))\n",
        "\n",
        "#         # Map emotions to numeric labels\n",
        "#         self.emotion_to_id = {\n",
        "#             'neutral': 0, 'calm': 1, 'happy': 2, 'sad': 3,\n",
        "#             'angry': 4, 'fearful': 5, 'disgust': 6, 'surprised': 7\n",
        "#         }\n",
        "#         self.labels_df[\"emotion_label\"] = self.labels_df[\"emotion_label\"].map(self.emotion_to_id)\n",
        "#         self.filename_to_label = dict(zip(self.labels_df[\"filename\"], self.labels_df[\"emotion_label\"]))\n",
        "\n",
        "#         # Precompute anatomical edge index\n",
        "#         self.edge_index = torch.tensor(self.get_anatomical_edge_list(), dtype=torch.long)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.files)\n",
        "\n",
        "#     def get_anatomical_edge_list(self):\n",
        "#         edge_index = []\n",
        "#         # Right iris\n",
        "#         for i in range(7): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((7, 0))\n",
        "#         # Right eye boundary\n",
        "#         for i in range(8, 19): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((19, 8))\n",
        "#         # Right pupil\n",
        "#         for i in range(20, 27): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((27, 20))\n",
        "#         # Left iris\n",
        "#         for i in range(28, 35): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((35, 28))\n",
        "#         # Left eye boundary\n",
        "#         for i in range(36, 47): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((41, 36))\n",
        "#         # Left pupil\n",
        "#         for i in range(48, 55): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((55, 48))\n",
        "#         # Jawline\n",
        "#         for i in range(56, 72): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((72, 56))\n",
        "#         # Right eyebrow\n",
        "#         for i in range(73, 77): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((77, 73))\n",
        "#         # Left eyebrow\n",
        "#         for i in range(78, 82): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((82, 78))\n",
        "#         # Nose bridge + lower nose\n",
        "#         for i in range(83, 91): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((91, 83))\n",
        "#         # Inner mouth\n",
        "#         for i in range(104, 115): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((115, 104))\n",
        "#         # Outer mouth\n",
        "#         for i in range(116, 123): edge_index.append((i, i + 1))\n",
        "#         edge_index.append((123, 116))\n",
        "#         # Additional anatomical connections\n",
        "#         edge_index += [(83, 92),(83, 98),(91, 110),(91, 120),(73, 92),(77, 97),\n",
        "#                        (78, 98),(82, 103),(56, 116),(72, 123),(92, 78),(98, 73),(83, 91),(110, 120)]\n",
        "#         return edge_index\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         filename = self.files[idx]\n",
        "#         file_path = os.path.join(self.data_dir, filename)\n",
        "\n",
        "#         with open(file_path, \"rb\") as f:\n",
        "#             video_data = pickle.load(f)\n",
        "\n",
        "#         frame_graphs = []\n",
        "#         mfcc_frames = []\n",
        "\n",
        "#         for frame in video_data:\n",
        "#             landmarks = np.array(frame['landmarks'], dtype=np.float32)\n",
        "#             landmarks -= landmarks.mean(axis=0)  # center landmarks\n",
        "#             mfcc = np.array(frame['mfcc'], dtype=np.float32)\n",
        "#             mfcc_frames.append(mfcc)\n",
        "\n",
        "#             x = torch.tensor(landmarks, dtype=torch.float)\n",
        "#             frame_graphs.append(Data(x=x, edge_index=self.edge_index))\n",
        "\n",
        "#         # Convert MFCC to tensor\n",
        "#         mfcc_tensor = torch.tensor(np.stack(mfcc_frames), dtype=torch.float)\n",
        "#         # Normalize per video\n",
        "#         mfcc_tensor = (mfcc_tensor - mfcc_tensor.mean(axis=0)) / (mfcc_tensor.std(axis=0) + 1e-6)\n",
        "\n",
        "#         label = torch.tensor(self.filename_to_label[filename], dtype=torch.long)\n",
        "#         seq_len = mfcc_tensor.size(0)\n",
        "#         return mfcc_tensor, frame_graphs, label, seq_len\n",
        "\n",
        "# # ---------------------------\n",
        "# # Collate function to handle variable-length sequences\n",
        "# # ---------------------------\n",
        "# def collate_fn(batch):\n",
        "#     mfccs, graphs, labels, lengths = zip(*batch)\n",
        "#     lengths = torch.tensor(lengths, dtype=torch.long)\n",
        "#     labels = torch.tensor(labels, dtype=torch.long)\n",
        "#     return mfccs, graphs, labels, lengths\n",
        "\n",
        "# # ---------------------------\n",
        "# # 2. Model\n",
        "# # ---------------------------\n",
        "# class LandmarkGraphEncoder(nn.Module):\n",
        "#     def __init__(self, in_dim, hidden_dim, out_dim, heads=4):\n",
        "#         super().__init__()\n",
        "#         self.gcn = GCNConv(in_dim, hidden_dim)\n",
        "#         self.gat = GATConv(hidden_dim, out_dim, heads=heads, concat=False)\n",
        "#     def forward(self, x, edge_index):\n",
        "#         x = self.gcn(x, edge_index)\n",
        "#         x = F.relu(x)\n",
        "#         x = self.gat(x, edge_index)\n",
        "#         return x\n",
        "\n",
        "# class AudioEncoder(nn.Module):\n",
        "#     def __init__(self, in_dim, out_dim):\n",
        "#         super().__init__()\n",
        "#         self.fc = nn.Linear(in_dim, out_dim)\n",
        "#     def forward(self, x):\n",
        "#         return F.relu(self.fc(x))\n",
        "\n",
        "# class CrossAttention(nn.Module):\n",
        "#     def __init__(self, d_model):\n",
        "#         super().__init__()\n",
        "#         self.query = nn.Linear(d_model, d_model)\n",
        "#         self.key = nn.Linear(d_model, d_model)\n",
        "#         self.value = nn.Linear(d_model, d_model)\n",
        "#     def forward(self, audio_emb, landmark_embs):\n",
        "#         Q = self.query(audio_emb).unsqueeze(1)\n",
        "#         K = self.key(landmark_embs)\n",
        "#         V = self.value(landmark_embs)\n",
        "#         attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (K.size(-1) ** 0.5)\n",
        "#         attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "#         out = torch.matmul(attn_weights, V)\n",
        "#         return out.squeeze(1)\n",
        "\n",
        "# class AudioVisualEmotionModel(nn.Module):\n",
        "#     def __init__(self, node_feat_dim, audio_feat_dim, hidden_dim, num_classes):\n",
        "#         super().__init__()\n",
        "#         self.landmark_gat = LandmarkGraphEncoder(node_feat_dim, hidden_dim, hidden_dim)\n",
        "#         self.audio_encoder = AudioEncoder(audio_feat_dim, hidden_dim)\n",
        "#         self.cross_attention = CrossAttention(hidden_dim)\n",
        "#         self.bilstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "#         self.fc = nn.Linear(hidden_dim*2, num_classes)\n",
        "\n",
        "#     def forward(self, mfcc_batch, adj, frame_graphs_list, lengths):\n",
        "#         batch_size = len(mfcc_batch)\n",
        "#         device = mfcc_batch[0].device\n",
        "#         hidden_dim = self.audio_encoder.fc.out_features\n",
        "\n",
        "#         frame_embeddings = []\n",
        "#         max_seq_len = max(lengths).item()\n",
        "\n",
        "#         for i in range(batch_size):\n",
        "#             frames = frame_graphs_list[i]\n",
        "#             seq_len = len(frames)\n",
        "#             video_emb = []\n",
        "#             for t in range(seq_len):\n",
        "#                 lf = frames[t].x.to(device)\n",
        "#                 af = mfcc_batch[i][t].unsqueeze(0)\n",
        "#                 lf_emb = self.landmark_gat(lf, adj)\n",
        "#                 af_emb = self.audio_encoder(af)\n",
        "#                 fused = self.cross_attention(af_emb, lf_emb)\n",
        "#                 video_emb.append(fused)\n",
        "#             video_emb = torch.cat(video_emb, dim=0)  # [seq_len, hidden_dim]\n",
        "#             # pad to max_seq_len\n",
        "#             if seq_len < max_seq_len:\n",
        "#                 pad = torch.zeros(max_seq_len - seq_len, hidden_dim, device=device)\n",
        "#                 video_emb = torch.cat([video_emb, pad], dim=0)\n",
        "#             frame_embeddings.append(video_emb)\n",
        "#         frame_embeddings = torch.stack(frame_embeddings, dim=0)  # [batch, max_seq_len, hidden_dim]\n",
        "\n",
        "#         # Pack sequence for LSTM\n",
        "#         packed = pack_padded_sequence(frame_embeddings, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "#         lstm_out, _ = self.bilstm(packed)\n",
        "#         lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True)\n",
        "\n",
        "#         # Mean pooling over valid frames\n",
        "#         outputs = []\n",
        "#         for i, seq_len in enumerate(lengths):\n",
        "#             pooled = lstm_out[i, :seq_len, :].mean(dim=0)\n",
        "#             outputs.append(pooled)\n",
        "#         outputs = torch.stack(outputs, dim=0)\n",
        "#         out = self.fc(outputs)\n",
        "#         return out\n",
        "\n",
        "# # ---------------------------\n",
        "# # 3. Stratified split helper\n",
        "# # ---------------------------\n",
        "# def stratified_split(labels_csv, test_size=0.2, random_state=42):\n",
        "#     df = pd.read_csv(labels_csv)\n",
        "#     train_files, val_files = train_test_split(\n",
        "#         df['filename'], test_size=test_size, stratify=df['emotion_label'], random_state=random_state\n",
        "#     )\n",
        "#     return train_files.tolist(), val_files.tolist()\n",
        "\n",
        "# # ---------------------------\n",
        "# # 4. Training / Validation\n",
        "# # ---------------------------\n",
        "# def train_epoch(model, loader, criterion, optimizer, device):\n",
        "#     model.train()\n",
        "#     total_loss = 0.0\n",
        "#     for mfccs, frame_graphs_list, labels, lengths in loader:\n",
        "#         labels = labels.to(device)\n",
        "#         mfccs = [f.to(device) for f in mfccs]\n",
        "#         adj = frame_graphs_list[0][0].edge_index.to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(mfccs, adj, frame_graphs_list, lengths)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item() * len(labels)\n",
        "#     return total_loss / len(loader.dataset)\n",
        "\n",
        "# def validate_epoch(model, loader, criterion, device):\n",
        "#     model.eval()\n",
        "#     total_loss = 0.0\n",
        "#     all_labels, all_preds = [], []\n",
        "#     with torch.no_grad():\n",
        "#         for mfccs, frame_graphs_list, labels, lengths in loader:\n",
        "#             labels = labels.to(device)\n",
        "#             mfccs = [f.to(device) for f in mfccs]\n",
        "#             adj = frame_graphs_list[0][0].edge_index.to(device)\n",
        "#             outputs = model(mfccs, adj, frame_graphs_list, lengths)\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             total_loss += loss.item() * len(labels)\n",
        "#             preds = torch.argmax(outputs, dim=1)\n",
        "#             all_labels.extend(labels.cpu().numpy())\n",
        "#             all_preds.extend(preds.cpu().numpy())\n",
        "#     acc = sum([p==t for p,t in zip(all_preds, all_labels)]) / len(all_labels)\n",
        "#     return total_loss / len(loader.dataset), acc, all_labels, all_preds\n",
        "\n",
        "# # ---------------------------\n",
        "# # 5. Main training script\n",
        "# # ---------------------------\n",
        "# def main():\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     data_dir = \"/content/drive/MyDrive/RAVDESS/Landmarks_WithAudio\"\n",
        "#     labels_csv = \"/content/drive/MyDrive/RAVDESS/emotion_labels.csv\"\n",
        "\n",
        "#     # Stratified split\n",
        "#     train_files, val_files = stratified_split(labels_csv)\n",
        "\n",
        "#     # Dataset\n",
        "#     full_dataset = RAVDESSDataset(data_dir, labels_csv)\n",
        "#     train_idx = [full_dataset.files.index(f) for f in train_files]\n",
        "#     val_idx = [full_dataset.files.index(f) for f in val_files]\n",
        "\n",
        "#     train_dataset = Subset(full_dataset, train_idx)\n",
        "#     val_dataset = Subset(full_dataset, val_idx)\n",
        "\n",
        "#     train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "#     val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "#     # Model\n",
        "#     model = AudioVisualEmotionModel(node_feat_dim=2, audio_feat_dim=40, hidden_dim=128, num_classes=8)\n",
        "#     model.to(device)\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "#     best_val_acc = 0.0\n",
        "#     num_epochs = 50\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "#         val_loss, val_acc, _, _ = validate_epoch(model, val_loader, criterion, device)\n",
        "#         print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "#         if val_acc > best_val_acc:\n",
        "#             best_val_acc = val_acc\n",
        "#             torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "#     # Final confusion matrix after training\n",
        "#     _, _, true_labels, pred_labels = validate_epoch(model, val_loader, criterion, device)\n",
        "#     cm = confusion_matrix(true_labels, pred_labels)\n",
        "#     disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(full_dataset.emotion_to_id.keys()))\n",
        "#     disp.plot(cmap=plt.cm.Blues)\n",
        "#     plt.show()\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "2ESVEKPhXjzA",
        "outputId": "e43cea04-9025-421a-ecfe-313ef325f1e7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Sizes of tensors must match except in dimension 1. Expected size 126 but got size 2 for tensor number 1 in the list.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3793652775.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3793652775.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3793652775.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmfccs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_graphs_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3793652775.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, mfcc_batch, adj, frame_graphs_list, lengths)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mlf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0maf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmfcc_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0mlf_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlandmark_gat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m                 \u001b[0maf_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mfused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maf_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlf_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3793652775.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGATConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_edge_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m                     edge_index, edge_weight = gcn_norm(  # yapf: disable\n\u001b[0m\u001b[1;32m    242\u001b[0m                         \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                         self.improved, self.add_self_loops, self.flow, x.dtype)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0madd_self_loops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         edge_index, edge_weight = add_remaining_self_loops(\n\u001b[0m\u001b[1;32m    100\u001b[0m             edge_index, edge_weight, fill_value, num_nodes)\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_geometric/utils/loop.py\u001b[0m in \u001b[0;36madd_remaining_self_loops\u001b[0;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0medge_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_undirected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_undirected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m     \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 126 but got size 2 for tensor number 1 in the list."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ---------------------------\n",
        "# # Imports\n",
        "# # ---------------------------\n",
        "# import os\n",
        "# import pickle\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from torch.utils.data import Dataset, DataLoader, Subset\n",
        "# from torch_geometric.data import Data\n",
        "# from torch_geometric.nn import GCNConv, GATConv\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "# from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# # ---------------------------\n",
        "# # 1. Dataset\n",
        "# # ---------------------------\n",
        "# class RAVDESSDataset(Dataset):\n",
        "#     def __init__(self, data_dir, labels_csv):\n",
        "#         self.data_dir = data_dir\n",
        "#         self.labels_df = pd.read_csv(labels_csv)\n",
        "#         self.files = sorted(os.listdir(data_dir))\n",
        "\n",
        "#         # Map emotions to numeric labels\n",
        "#         self.emotion_to_id = {\n",
        "#             'neutral': 0, 'calm': 1, 'happy': 2, 'sad': 3,\n",
        "#             'angry': 4, 'fearful': 5, 'disgust': 6, 'surprised': 7\n",
        "#         }\n",
        "#         self.labels_df[\"emotion_label\"] = self.labels_df[\"emotion_label\"].map(self.emotion_to_id)\n",
        "#         self.filename_to_label = dict(zip(self.labels_df[\"filename\"], self.labels_df[\"emotion_label\"]))\n",
        "\n",
        "#         # Precompute anatomical edge index as [2, num_edges] tensor\n",
        "#         self.edge_index = torch.tensor(self.get_anatomical_edge_list(), dtype=torch.long).t().contiguous()\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.files)\n",
        "\n",
        "#     def get_anatomical_edge_list(self):\n",
        "#         edges = []\n",
        "#         # Right iris\n",
        "#         for i in range(7): edges.append((i, i+1))\n",
        "#         edges.append((7, 0))\n",
        "#         # Right eye boundary\n",
        "#         for i in range(8, 19): edges.append((i, i+1))\n",
        "#         edges.append((19, 8))\n",
        "#         # Right pupil\n",
        "#         for i in range(20, 27): edges.append((i, i+1))\n",
        "#         edges.append((27, 20))\n",
        "#         # Left iris\n",
        "#         for i in range(28, 35): edges.append((i, i+1))\n",
        "#         edges.append((35, 28))\n",
        "#         # Left eye boundary\n",
        "#         for i in range(36, 47): edges.append((i, i+1))\n",
        "#         edges.append((41, 36))\n",
        "#         # Left pupil\n",
        "#         for i in range(48, 55): edges.append((i, i+1))\n",
        "#         edges.append((55, 48))\n",
        "#         # Jawline\n",
        "#         for i in range(56, 72): edges.append((i, i+1))\n",
        "#         edges.append((72, 56))\n",
        "#         # Right eyebrow\n",
        "#         for i in range(73, 77): edges.append((i, i+1))\n",
        "#         edges.append((77, 73))\n",
        "#         # Left eyebrow\n",
        "#         for i in range(78, 82): edges.append((i, i+1))\n",
        "#         edges.append((82, 78))\n",
        "#         # Nose bridge + lower nose\n",
        "#         for i in range(83, 91): edges.append((i, i+1))\n",
        "#         edges.append((91, 83))\n",
        "#         # Inner mouth\n",
        "#         for i in range(104, 115): edges.append((i, i+1))\n",
        "#         edges.append((115, 104))\n",
        "#         # Outer mouth\n",
        "#         for i in range(116, 123): edges.append((i, i+1))\n",
        "#         edges.append((123, 116))\n",
        "#         # Additional anatomical connections\n",
        "#         edges += [(83,92),(83,98),(91,110),(91,120),(73,92),(77,97),\n",
        "#                   (78,98),(82,103),(56,116),(72,123),(92,78),(98,73),(83,91),(110,120)]\n",
        "#         return edges\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         filename = self.files[idx]\n",
        "#         file_path = os.path.join(self.data_dir, filename)\n",
        "\n",
        "#         with open(file_path, \"rb\") as f:\n",
        "#             video_data = pickle.load(f)\n",
        "\n",
        "#         frame_graphs = []\n",
        "#         mfcc_frames = []\n",
        "\n",
        "#         for frame in video_data:\n",
        "#             landmarks = np.array(frame['landmarks'], dtype=np.float32)\n",
        "#             landmarks -= landmarks.mean(axis=0)  # center landmarks\n",
        "#             mfcc = np.array(frame['mfcc'], dtype=np.float32)\n",
        "#             mfcc_frames.append(mfcc)\n",
        "\n",
        "#             x = torch.tensor(landmarks, dtype=torch.float)\n",
        "#             frame_graphs.append(Data(x=x, edge_index=self.edge_index))\n",
        "\n",
        "#         mfcc_tensor = torch.tensor(np.stack(mfcc_frames), dtype=torch.float)\n",
        "#         # Normalize per video\n",
        "#         mfcc_tensor = (mfcc_tensor - mfcc_tensor.mean(axis=0)) / (mfcc_tensor.std(axis=0) + 1e-6)\n",
        "\n",
        "#         label = torch.tensor(self.filename_to_label[filename], dtype=torch.long)\n",
        "#         seq_len = mfcc_tensor.size(0)\n",
        "#         return mfcc_tensor, frame_graphs, label, seq_len\n",
        "\n",
        "# # Collate function to handle variable-length sequences\n",
        "# def collate_fn(batch):\n",
        "#     mfccs, graphs, labels, lengths = zip(*batch)\n",
        "#     lengths = torch.tensor(lengths, dtype=torch.long)\n",
        "#     labels = torch.tensor(labels, dtype=torch.long)\n",
        "#     return mfccs, graphs, labels, lengths\n",
        "\n",
        "# # ---------------------------\n",
        "# # 2. Model\n",
        "# # ---------------------------\n",
        "# class LandmarkGraphEncoder(nn.Module):\n",
        "#     def __init__(self, in_dim, hidden_dim, out_dim, heads=4):\n",
        "#         super().__init__()\n",
        "#         self.gcn = GCNConv(in_dim, hidden_dim)\n",
        "#         self.gat = GATConv(hidden_dim, out_dim, heads=heads, concat=False)\n",
        "#     def forward(self, x, edge_index):\n",
        "#         x = self.gcn(x, edge_index)\n",
        "#         x = F.relu(x)\n",
        "#         x = self.gat(x, edge_index)\n",
        "#         return x\n",
        "\n",
        "# class AudioEncoder(nn.Module):\n",
        "#     def __init__(self, in_dim, out_dim):\n",
        "#         super().__init__()\n",
        "#         self.fc = nn.Linear(in_dim, out_dim)\n",
        "#     def forward(self, x):\n",
        "#         return F.relu(self.fc(x))\n",
        "\n",
        "# class CrossAttention(nn.Module):\n",
        "#     def __init__(self, d_model):\n",
        "#         super().__init__()\n",
        "#         self.query = nn.Linear(d_model, d_model)\n",
        "#         self.key = nn.Linear(d_model, d_model)\n",
        "#         self.value = nn.Linear(d_model, d_model)\n",
        "#     def forward(self, audio_emb, landmark_embs):\n",
        "#         Q = self.query(audio_emb).unsqueeze(1)          # [batch,1,d_model]\n",
        "#         K = self.key(landmark_embs)                     # [num_nodes,d_model]\n",
        "#         V = self.value(landmark_embs)\n",
        "#         attn_scores = torch.matmul(Q, K.transpose(-2,-1)) / (K.size(-1)**0.5)\n",
        "#         attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "#         out = torch.matmul(attn_weights, V)\n",
        "#         return out.squeeze(1)\n",
        "\n",
        "# class AudioVisualEmotionModel(nn.Module):\n",
        "#     def __init__(self, node_feat_dim, audio_feat_dim, hidden_dim, num_classes):\n",
        "#         super().__init__()\n",
        "#         self.landmark_gat = LandmarkGraphEncoder(node_feat_dim, hidden_dim, hidden_dim)\n",
        "#         self.audio_encoder = AudioEncoder(audio_feat_dim, hidden_dim)\n",
        "#         self.cross_attention = CrossAttention(hidden_dim)\n",
        "#         self.bilstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "#         self.fc = nn.Linear(hidden_dim*2, num_classes)\n",
        "\n",
        "#     def forward(self, mfcc_batch, adj, frame_graphs_list, lengths):\n",
        "#         batch_size = len(mfcc_batch)\n",
        "#         device = mfcc_batch[0].device\n",
        "#         hidden_dim = self.audio_encoder.fc.out_features\n",
        "#         frame_embeddings = []\n",
        "#         max_seq_len = max(lengths).item()\n",
        "\n",
        "#         for i in range(batch_size):\n",
        "#             frames = frame_graphs_list[i]\n",
        "#             seq_len = len(frames)\n",
        "#             video_emb = []\n",
        "#             for t in range(seq_len):\n",
        "#                 lf = frames[t].x.to(device)\n",
        "#                 af = mfcc_batch[i][t].unsqueeze(0).to(device)\n",
        "#                 lf_emb = self.landmark_gat(lf, adj)\n",
        "#                 af_emb = self.audio_encoder(af)\n",
        "#                 fused = self.cross_attention(af_emb, lf_emb)\n",
        "#                 video_emb.append(fused)\n",
        "#             video_emb = torch.cat(video_emb, dim=0)  # [seq_len, hidden_dim]\n",
        "#             if seq_len < max_seq_len:                 # pad\n",
        "#                 pad = torch.zeros(max_seq_len - seq_len, hidden_dim, device=device)\n",
        "#                 video_emb = torch.cat([video_emb, pad], dim=0)\n",
        "#             frame_embeddings.append(video_emb)\n",
        "\n",
        "#         frame_embeddings = torch.stack(frame_embeddings, dim=0)  # [batch,max_seq_len,hidden_dim]\n",
        "#         packed = pack_padded_sequence(frame_embeddings, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "#         lstm_out, _ = self.bilstm(packed)\n",
        "#         lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True)\n",
        "\n",
        "#         outputs = []\n",
        "#         for i, seq_len in enumerate(lengths):\n",
        "#             pooled = lstm_out[i, :seq_len, :].mean(dim=0)\n",
        "#             outputs.append(pooled)\n",
        "#         outputs = torch.stack(outputs, dim=0)\n",
        "#         out = self.fc(outputs)\n",
        "#         return out\n",
        "\n",
        "# # ---------------------------\n",
        "# # 3. Stratified split\n",
        "# # ---------------------------\n",
        "# def stratified_split(labels_csv, test_size=0.2, random_state=42):\n",
        "#     df = pd.read_csv(labels_csv)\n",
        "#     train_files, val_files = train_test_split(\n",
        "#         df['filename'], test_size=test_size, stratify=df['emotion_label'], random_state=random_state\n",
        "#     )\n",
        "#     return train_files.tolist(), val_files.tolist()\n",
        "\n",
        "# # ---------------------------\n",
        "# # 4. Train / Validate\n",
        "# # ---------------------------\n",
        "# def train_epoch(model, loader, criterion, optimizer, device):\n",
        "#     model.train()\n",
        "#     total_loss = 0.0\n",
        "#     for mfccs, frame_graphs_list, labels, lengths in loader:\n",
        "#         labels = labels.to(device)\n",
        "#         mfccs = [f.to(device) for f in mfccs]\n",
        "#         adj = frame_graphs_list[0][0].edge_index.to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(mfccs, adj, frame_graphs_list, lengths)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item() * len(labels)\n",
        "#     return total_loss / len(loader.dataset)\n",
        "\n",
        "# def validate_epoch(model, loader, criterion, device):\n",
        "#     model.eval()\n",
        "#     total_loss = 0.0\n",
        "#     all_labels, all_preds = [], []\n",
        "#     with torch.no_grad():\n",
        "#         for mfccs, frame_graphs_list, labels, lengths in loader:\n",
        "#             labels = labels.to(device)\n",
        "#             mfccs = [f.to(device) for f in mfccs]\n",
        "#             adj = frame_graphs_list[0][0].edge_index.to(device)\n",
        "#             outputs = model(mfccs, adj, frame_graphs_list, lengths)\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             total_loss += loss.item() * len(labels)\n",
        "#             preds = torch.argmax(outputs, dim=1)\n",
        "#             all_labels.extend(labels.cpu().numpy())\n",
        "#             all_preds.extend(preds.cpu().numpy())\n",
        "#     acc = sum([p==t for p,t in zip(all_preds, all_labels)]) / len(all_labels)\n",
        "#     return total_loss / len(loader.dataset), acc, all_labels, all_preds\n",
        "\n",
        "# # ---------------------------\n",
        "# # 5. Main\n",
        "# # ---------------------------\n",
        "# def main():\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     data_dir = \"/content/drive/MyDrive/RAVDESS/Landmarks_WithAudio\"\n",
        "#     labels_csv = \"/content/drive/MyDrive/RAVDESS/emotion_labels.csv\"\n",
        "\n",
        "#     train_files, val_files = stratified_split(labels_csv)\n",
        "#     full_dataset = RAVDESSDataset(data_dir, labels_csv)\n",
        "\n",
        "#     train_idx = [full_dataset.files.index(f) for f in train_files]\n",
        "#     val_idx = [full_dataset.files.index(f) for f in val_files]\n",
        "\n",
        "#     train_dataset = Subset(full_dataset, train_idx)\n",
        "#     val_dataset = Subset(full_dataset, val_idx)\n",
        "\n",
        "#     train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "#     val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "#     model = AudioVisualEmotionModel(node_feat_dim=2, audio_feat_dim=40, hidden_dim=128, num_classes=8)\n",
        "#     model.to(device)\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "#     best_val_acc = 0.0\n",
        "#     num_epochs = 50\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "#         val_loss, val_acc, _, _ = validate_epoch(model, val_loader, criterion, device)\n",
        "#         print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "#         if val_acc > best_val_acc:\n",
        "#             best_val_acc = val_acc\n",
        "#             torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "#     # Final confusion matrix\n",
        "#     _, _, true_labels, pred_labels = validate_epoch(model, val_loader, criterion, device)\n",
        "#     cm = confusion_matrix(true_labels, pred_labels)\n",
        "#     disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(full_dataset.emotion_to_id.keys()))\n",
        "#     disp.plot(cmap=plt.cm.Blues)\n",
        "#     plt.show()\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpeNUfvRlBOw",
        "outputId": "24a0e105-fcac-408a-c9c8-e29998f774cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50] Train Loss: 2.0609 | Val Loss: 2.0403 | Val Acc: 0.1527\n",
            "Epoch [2/50] Train Loss: 2.0505 | Val Loss: 2.0528 | Val Acc: 0.1527\n",
            "Epoch [3/50] Train Loss: 2.0940 | Val Loss: 2.0514 | Val Acc: 0.1527\n",
            "Epoch [4/50] Train Loss: 2.0755 | Val Loss: 2.0533 | Val Acc: 0.1527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# FULL AUDIO-VISUAL EMOTION RECOGNITION SCRIPT\n",
        "# ==========================================\n",
        "\n",
        "# ---------------------------\n",
        "# Imports\n",
        "# ---------------------------\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv, GATConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Dataset\n",
        "# ---------------------------\n",
        "class RAVDESSDataset(Dataset):\n",
        "    def __init__(self, data_dir, labels_csv):\n",
        "        self.data_dir = data_dir\n",
        "        self.labels_df = pd.read_csv(labels_csv)\n",
        "        self.files = sorted(os.listdir(data_dir))\n",
        "\n",
        "        # Map emotions to numeric labels\n",
        "        self.emotion_to_id = {\n",
        "            'neutral': 0, 'calm': 1, 'happy': 2, 'sad': 3,\n",
        "            'angry': 4, 'fearful': 5, 'disgust': 6, 'surprised': 7\n",
        "        }\n",
        "        self.labels_df[\"emotion_label\"] = self.labels_df[\"emotion_label\"].map(self.emotion_to_id)\n",
        "        self.filename_to_label = dict(zip(self.labels_df[\"filename\"], self.labels_df[\"emotion_label\"]))\n",
        "\n",
        "        # Precompute anatomical edge index\n",
        "        self.edge_index = torch.tensor(self.get_anatomical_edge_list(), dtype=torch.long).t().contiguous()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def get_anatomical_edge_list(self):\n",
        "        edge_index = []\n",
        "        # Right iris\n",
        "        for i in range(7): edge_index.append((i, i + 1))\n",
        "        edge_index.append((7, 0))\n",
        "        # Right eye boundary\n",
        "        for i in range(8, 19): edge_index.append((i, i + 1))\n",
        "        edge_index.append((19, 8))\n",
        "        # Right pupil\n",
        "        for i in range(20, 27): edge_index.append((i, i + 1))\n",
        "        edge_index.append((27, 20))\n",
        "        # Left iris\n",
        "        for i in range(28, 35): edge_index.append((i, i + 1))\n",
        "        edge_index.append((35, 28))\n",
        "        # Left eye boundary\n",
        "        for i in range(36, 47): edge_index.append((i, i + 1))\n",
        "        edge_index.append((41, 36))\n",
        "        # Left pupil\n",
        "        for i in range(48, 55): edge_index.append((i, i + 1))\n",
        "        edge_index.append((55, 48))\n",
        "        # Jawline\n",
        "        for i in range(56, 72): edge_index.append((i, i + 1))\n",
        "        edge_index.append((72, 56))\n",
        "        # Right eyebrow\n",
        "        for i in range(73, 77): edge_index.append((i, i + 1))\n",
        "        edge_index.append((77, 73))\n",
        "        # Left eyebrow\n",
        "        for i in range(78, 82): edge_index.append((i, i + 1))\n",
        "        edge_index.append((82, 78))\n",
        "        # Nose bridge + lower nose\n",
        "        for i in range(83, 91): edge_index.append((i, i + 1))\n",
        "        edge_index.append((91, 83))\n",
        "        # Inner mouth\n",
        "        for i in range(104, 115): edge_index.append((i, i + 1))\n",
        "        edge_index.append((115, 104))\n",
        "        # Outer mouth\n",
        "        for i in range(116, 123): edge_index.append((i, i + 1))\n",
        "        edge_index.append((123, 116))\n",
        "        # Additional anatomical connections\n",
        "        edge_index += [(83, 92),(83, 98),(91, 110),(91, 120),(73, 92),(77, 97),\n",
        "                       (78, 98),(82, 103),(56, 116),(72, 123),(92, 78),(98, 73),(83, 91),(110, 120)]\n",
        "        return edge_index\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.files[idx]\n",
        "        file_path = os.path.join(self.data_dir, filename)\n",
        "\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            video_data = pickle.load(f)\n",
        "\n",
        "        frame_graphs = []\n",
        "        mfcc_frames = []\n",
        "\n",
        "        for frame in video_data:\n",
        "            landmarks = np.array(frame['landmarks'], dtype=np.float32)\n",
        "            landmarks -= landmarks.mean(axis=0)  # center landmarks\n",
        "            mfcc = np.array(frame['mfcc'], dtype=np.float32)\n",
        "            mfcc_frames.append(mfcc)\n",
        "\n",
        "            x = torch.tensor(landmarks, dtype=torch.float)\n",
        "            frame_graphs.append(Data(x=x, edge_index=self.edge_index))\n",
        "\n",
        "        # Convert MFCC to tensor\n",
        "        mfcc_tensor = torch.tensor(np.stack(mfcc_frames), dtype=torch.float)\n",
        "        # Normalize per video\n",
        "        mfcc_tensor = (mfcc_tensor - mfcc_tensor.mean(axis=0)) / (mfcc_tensor.std(axis=0) + 1e-6)\n",
        "\n",
        "        label = torch.tensor(self.filename_to_label[filename], dtype=torch.long)\n",
        "        seq_len = mfcc_tensor.size(0)\n",
        "        return mfcc_tensor, frame_graphs, label, seq_len\n",
        "\n",
        "# ---------------------------\n",
        "# Collate function\n",
        "# ---------------------------\n",
        "def collate_fn(batch):\n",
        "    mfccs, graphs, labels, lengths = zip(*batch)\n",
        "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    return mfccs, graphs, labels, lengths\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Model\n",
        "# ---------------------------\n",
        "class LandmarkGraphEncoder(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, heads=4):\n",
        "        super().__init__()\n",
        "        self.gcn = GCNConv(in_dim, hidden_dim)\n",
        "        self.gat = GATConv(hidden_dim, out_dim, heads=heads, concat=False)\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.gcn(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.gat(x, edge_index)\n",
        "        return x\n",
        "\n",
        "class AudioEncoder(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(in_dim, out_dim)\n",
        "    def forward(self, x):\n",
        "        return F.relu(self.fc(x))\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "    def forward(self, audio_emb, landmark_embs):\n",
        "        Q = self.query(audio_emb).unsqueeze(1)\n",
        "        K = self.key(landmark_embs)\n",
        "        V = self.value(landmark_embs)\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (K.size(-1) ** 0.5)\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        out = torch.matmul(attn_weights, V)\n",
        "        return out.squeeze(1)\n",
        "\n",
        "class TemporalAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn_fc = nn.Linear(hidden_dim*2, 1)\n",
        "    def forward(self, lstm_out, lengths):\n",
        "        attn_scores = self.attn_fc(lstm_out).squeeze(-1)\n",
        "        mask = torch.arange(lstm_out.size(1))[None, :].to(lengths.device) < lengths[:, None]\n",
        "        attn_scores[~mask] = float('-inf')\n",
        "        attn_weights = torch.softmax(attn_scores, dim=1).unsqueeze(-1)\n",
        "        weighted_sum = (lstm_out * attn_weights).sum(dim=1)\n",
        "        return weighted_sum\n",
        "\n",
        "class AudioVisualEmotionModel(nn.Module):\n",
        "    def __init__(self, node_feat_dim, audio_feat_dim, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.landmark_gat = LandmarkGraphEncoder(node_feat_dim, hidden_dim, hidden_dim)\n",
        "        self.audio_encoder = AudioEncoder(audio_feat_dim, hidden_dim)\n",
        "        self.cross_attention = CrossAttention(hidden_dim)\n",
        "        self.bilstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.temporal_attn = TemporalAttention(hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim*2, num_classes)\n",
        "\n",
        "    def forward(self, mfcc_batch, adj, frame_graphs_list, lengths):\n",
        "        batch_size = len(mfcc_batch)\n",
        "        device = mfcc_batch[0].device\n",
        "        hidden_dim = self.audio_encoder.fc.out_features\n",
        "\n",
        "        frame_embeddings = []\n",
        "        max_seq_len = max(lengths).item()\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            frames = frame_graphs_list[i]\n",
        "            seq_len = len(frames)\n",
        "            video_emb = []\n",
        "            for t in range(seq_len):\n",
        "                lf = frames[t].x.to(device)\n",
        "                af = mfcc_batch[i][t].unsqueeze(0)\n",
        "                lf_emb = self.landmark_gat(lf, adj)\n",
        "                af_emb = self.audio_encoder(af)\n",
        "                fused = self.cross_attention(af_emb, lf_emb)\n",
        "                video_emb.append(fused)\n",
        "            video_emb = torch.cat(video_emb, dim=0)\n",
        "            if seq_len < max_seq_len:\n",
        "                pad = torch.zeros(max_seq_len - seq_len, hidden_dim, device=device)\n",
        "                video_emb = torch.cat([video_emb, pad], dim=0)\n",
        "            frame_embeddings.append(video_emb)\n",
        "\n",
        "        frame_embeddings = torch.stack(frame_embeddings, dim=0)\n",
        "        packed = pack_padded_sequence(frame_embeddings, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        lstm_out, _ = self.bilstm(packed)\n",
        "        lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True)\n",
        "        video_embeds = self.temporal_attn(lstm_out, lengths)\n",
        "        out = self.fc(video_embeds)\n",
        "        return out\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Train / Validation Functions\n",
        "# ---------------------------\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for mfccs, frame_graphs_list, labels, lengths in loader:\n",
        "        labels = labels.to(device)\n",
        "        mfccs = [f.to(device) for f in mfccs]\n",
        "        adj = frame_graphs_list[0][0].edge_index.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(mfccs, adj, frame_graphs_list, lengths)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * len(labels)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def validate_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    all_labels, all_preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for mfccs, frame_graphs_list, labels, lengths in loader:\n",
        "            labels = labels.to(device)\n",
        "            mfccs = [f.to(device) for f in mfccs]\n",
        "            adj = frame_graphs_list[0][0].edge_index.to(device)\n",
        "            outputs = model(mfccs, adj, frame_graphs_list, lengths)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * len(labels)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "    acc = sum([p==t for p,t in zip(all_preds, all_labels)]) / len(all_labels)\n",
        "    return total_loss / len(loader.dataset), acc, all_labels, all_preds\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Main Script\n",
        "# ---------------------------\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    data_dir = \"/content/drive/MyDrive/RAVDESS/Landmarks_WithAudio\"\n",
        "    labels_csv = \"/content/drive/MyDrive/RAVDESS/emotion_labels.csv\"\n",
        "\n",
        "    full_dataset = RAVDESSDataset(data_dir, labels_csv)\n",
        "\n",
        "    train_files, val_files = train_test_split(\n",
        "        full_dataset.files, test_size=0.2, stratify=full_dataset.labels_df[\"emotion_label\"], random_state=42\n",
        "    )\n",
        "    train_idx = [full_dataset.files.index(f) for f in train_files]\n",
        "    val_idx = [full_dataset.files.index(f) for f in val_files]\n",
        "\n",
        "    train_dataset = Subset(full_dataset, train_idx)\n",
        "    val_dataset = Subset(full_dataset, val_idx)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    model = AudioVisualEmotionModel(node_feat_dim=2, audio_feat_dim=40, hidden_dim=64, num_classes=8)\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    num_epochs = 50\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc, _, _ = validate_epoch(model, val_loader, criterion, device)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "    # Confusion matrix\n",
        "    _, _, true_labels, pred_labels = validate_epoch(model, val_loader, criterion, device)\n",
        "    cm = confusion_matrix(true_labels, pred_labels)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(full_dataset.emotion_to_id.keys()))\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtI2gX1P2g25",
        "outputId": "26542231-b9cf-4686-b2ae-305243ab43fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50] Train Loss: 2.0257 | Val Loss: 1.9474 | Val Acc: 0.1894\n",
            "Epoch [2/50] Train Loss: 1.8880 | Val Loss: 1.8066 | Val Acc: 0.2668\n",
            "Epoch [3/50] Train Loss: 1.7406 | Val Loss: 1.7360 | Val Acc: 0.3340\n",
            "Epoch [4/50] Train Loss: 1.6448 | Val Loss: 1.7291 | Val Acc: 0.3116\n",
            "Epoch [5/50] Train Loss: 1.5798 | Val Loss: 1.6017 | Val Acc: 0.3910\n",
            "Epoch [6/50] Train Loss: 1.5097 | Val Loss: 1.5991 | Val Acc: 0.3666\n",
            "Epoch [7/50] Train Loss: 1.4622 | Val Loss: 1.5353 | Val Acc: 0.4134\n",
            "Epoch [8/50] Train Loss: 1.4189 | Val Loss: 1.5047 | Val Acc: 0.4257\n",
            "Epoch [9/50] Train Loss: 1.3642 | Val Loss: 1.5249 | Val Acc: 0.4033\n",
            "Epoch [10/50] Train Loss: 1.3355 | Val Loss: 1.5001 | Val Acc: 0.4358\n",
            "Epoch [11/50] Train Loss: 1.3185 | Val Loss: 1.4544 | Val Acc: 0.4134\n",
            "Epoch [12/50] Train Loss: 1.2650 | Val Loss: 1.4728 | Val Acc: 0.4297\n",
            "Epoch [13/50] Train Loss: 1.2455 | Val Loss: 1.4758 | Val Acc: 0.4358\n"
          ]
        }
      ]
    }
  ]
}